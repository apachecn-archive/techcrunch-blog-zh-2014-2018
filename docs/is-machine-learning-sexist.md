# 机器学习是性别歧视吗？

> 原文：<https://web.archive.org/web/https://techcrunch.com/2016/10/11/is-machine-learning-sexist/>

我们最近听到了很多关于多样性和包容性的说法，许多人特别感兴趣和兴奋的领域之一是应用[机器学习来消除偏见](https://web.archive.org/web/20230316161016/http://news.sap.com/sap-reveals-new-technology-designed-to-help-move-business-beyond-bias/)。

如果做得好，这将是一个巨大的推动，支持我们在组织的所有领域超越偏见的努力。但是也有潜在的陷阱；如果处理不当，机器学习实际上会让你的业务*更加*有偏见。

我们来看看谷歌的 word2vec，比如。使用数百万的谷歌新闻数据集，谷歌研究人员提取了彼此相关的词汇模式。通过在向量空间中表示术语，他们能够用简单的向量代数推断单词之间的关系。

例如，系统可以回答诸如“姐妹对于女人就像兄弟对于什么？”(姐姐:女人::哥哥:？)正确地用“人。”

但这些规则的挑战就在这里:因为系统是用现有新闻训练的，它也会遵循那些文章中的偏见。而在谷歌新闻集中，这些文章被证明是令人震惊的偏见。

例如，如果你输入“父亲:医生::母亲:？”上面写着“护士”

对于“男人:电脑程序员::女人:？”它会给你“家庭主妇”

那么，这是否意味着机器学习是性别歧视呢？不。但这个机器学习的例子无情地暴露了我们今天的新闻业和记者中仍然存在的偏见。从统计学的角度来看，仅从文章中得出的结论是正确的。但文章本身明显有失偏颇。

同样，如果你的组织中存在偏见，无论是雇佣、发展还是晋升的方式，仅仅将现有数据作为机器学习的基础实际上可能会实现与你试图实现的相反的结果——这意味着它可能会加强和放大偏见，而不是消除它。如果你总是提拔男性，系统很可能会把身为男性视为某人获得提拔的预测因素。

谷歌搜索遇到了这样的问题，当它向男性显示比女性更有声望的职位时，或者当它只在搜索“首席执行官”的第一页显示男性图像时。

这些例子以及许多其他例子表明，偏见甚至可以渗透到旨在消除偏见的算法中。这些算法并非故意糟糕，但它们要么反映了编写它们的人的偏见，要么反映了用于训练系统的底层数据的偏见。

因此，需要对软件创作本身进行根本性的反思和谨慎的处理。必须格外小心地培训创建系统的人，并将研究纳入机器学习算法，这样你就不会意外地产生更多的偏见。这个系统必须被重新训练以不同的方式思考，就像你的员工一样。

最初的机器学习实验可能有助于暴露偏见，但如果不加修改，实际上可能会让你的问题变得更糟，而不是消除它。因此，在表面上接受所有机器学习之前，请将原始结果视为现有偏见的指标，并确保您的系统是在没有偏见的[业务中创建的](https://web.archive.org/web/20230316161016/http://news.sap.com/sap-reveals-new-technology-designed-to-help-move-business-beyond-bias/)，使用没有偏见的数据。