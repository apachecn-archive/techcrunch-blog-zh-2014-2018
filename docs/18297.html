<html>
<head>
<title>Google’s new ‘AI principles’ forbid its use in weapons and human rights violations – Page 100 – TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌新的“人工智能原则”禁止其用于武器和侵犯人权——第100页——TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/06/07/googles-new-ai-principles-forbid-its-use-in-weapons-and-human-rights-violations/100/">https://web.archive.org/web/https://techcrunch.com/2018/06/07/googles-new-ai-principles-forbid-its-use-in-weapons-and-human-rights-violations/100/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><a class="crunchbase-link" href="https://web.archive.org/web/20190923011009/https://crunchbase.com/organization/google" target="_blank" data-type="organization" data-entity="google">谷歌<span class="crunchbase-tooltip-indicator"/> </a>已经<a href="https://web.archive.org/web/20190923011009/https://www.blog.google/topics/ai/ai-principles/">发布了一套模糊但令人钦佩的“人工智能原则”</a>解释了它将会和不会在该领域部署其巨大影响力的方式。“这些不是理论概念；他们是具体的标准，将积极管理我们的研究和产品开发，并将影响我们的商业决策，”首席执行官桑德尔·皮帅写道。</p>
<p class="translated">这些原则是在围绕Project Maven的几个月低层次争议之后制定的，该项目是与美国军方签订的一份合同，涉及无人机镜头的图像分析。一些员工反对这项工作，甚至辞职以示抗议，但这个问题实际上是对人工智能以及如何能够和应该使用人工智能的焦虑的缩影。</p>
<p class="translated">与皮查伊关于这些原则具有约束力的断言一致，谷歌云首席执行官黛安·格林今天<a href="https://web.archive.org/web/20190923011009/https://www.blog.google/topics/google-cloud/incorporating-googles-ai-principles-google-cloud/">在另一篇文章</a>中证实了上周<a href="https://web.archive.org/web/20190923011009/https://techcrunch.com/2018/06/01/google-reportedly-backing-out-of-military-contract-after-public-backlash/">的传言</a>，即所涉合同将不会续约或与他人续约。有报道称，谷歌正在利用Project Maven作为一种手段，来获得更有利可图和更敏感的政府合同所需的安全许可，但这一报道没有得到解决。</p>
<p class="translated">原则本身如下，相关部分引自其描述:</p>
<ol>
<li class="translated"><strong>对社会有益:</strong>考虑广泛的社会和经济因素，并在我们认为总体可能收益大大超过可预见的风险和不利因素的情况下继续进行……同时继续尊重我们运营所在国家的文化、社会和法律规范。</li>
<li class="translated"><strong>避免产生或强化不公平的偏见:</strong>避免对人们产生不公平的影响，尤其是那些与敏感特征相关的影响，如种族、民族、性别、国籍、收入、性取向、能力、政治或宗教信仰。</li>
<li class="translated"><strong>针对安全性进行构建和测试:</strong>应用强有力的安全和安保实践，以避免导致伤害风险的意外结果。</li>
<li class="translated"><strong>对人负责:</strong>为反馈、相关解释和申诉提供适当的机会。</li>
<li class="translated"><strong>纳入隐私设计原则:</strong>给予通知和同意的机会，鼓励具有隐私保护措施的架构，并对数据的使用提供适当的透明度和控制。</li>
<li class="translated"><strong>坚持科学卓越的高标准:</strong>与一系列利益相关方合作，利用科学严谨和多学科的方法，促进该领域的深思熟虑的领导力……通过发布教育材料、最佳实践和研究，负责任地分享人工智能知识，使更多人能够开发有用的人工智能应用。</li>
<li class="translated"><strong>可用于符合这些原则的用途:</strong>限制潜在的有害或滥用用途。(规模、独特性、主要目的和谷歌的角色是评估这一点的因素。)</li>
</ol>
<p class="translated">除了陈述公司<em>会</em>做什么，皮柴还概述了它<em>不会</em>做什么。具体来说，谷歌不会在以下领域追求或部署人工智能:</p>
<ul>
<li class="translated">造成或可能造成整体损害的技术。(根据风险/收益分析。)</li>
<li class="translated">其主要目的或实施是造成或直接便利对人的伤害的武器或其他技术。</li>
<li class="translated">收集或使用信息进行监视的技术违反了国际公认的规范。</li>
<li class="translated">其目的违反公认的国际法和人权原则的技术。</li>
</ul>
<p class="translated">(不提为恶。)</p>
<p class="translated">在七项原则及其描述中，谷歌给自己留了相当大的余地，可以自由使用“适当”这样的词。什么时候是反馈的“合适”时机？什么是“适当的”人类方向和控制？“适当的”安全约束怎么样？</p>
<p class="translated">在如此短的时间内，期望有这样的硬性规定是过分的，这是有争议的，但我认为这实际上不是短时间的通知；谷歌多年来一直是人工智能领域的领导者，有大量时间来建立不仅仅是原则。</p>
<p class="translated">例如，其“尊重文化、社会和法律规范”的承诺已经在许多方面得到了验证。我们在哪里可以看到不顾这些规范的做法被应用，或者谷歌的政策已经倾向于适应政府或宗教权威的要求？</p>
<p class="translated">在承诺避免产生偏见并对人们负责时，肯定(基于谷歌在这里的现有工作)有一些具体的东西要说吗？例如，如果任何涉及谷歌的系统有基于敏感数据或类别的结果，该系统将是完全可审计的，并可供公众关注？</p>
<p class="translated">这里的想法值得称赞，但AI的应用并不抽象；这些系统今天被用于决定警力部署，或选择房屋贷款利率，或分析医疗数据。真正的规则是需要的，如果谷歌真的打算保持其在该领域的领导者地位，它必须建立这些规则，或者，如果已经建立了规则，就在显著位置公布它们。</p>
<p class="translated">最终，可能是谷歌不会做的事情清单变得更短，更具限制性。虽然在原则中使用“适当的”允许公司有解释的空间，但相反的情况适用于禁止的追求的定义。这些定义非常不确定，监管机构对“可能造成整体伤害”或“国际公认规范”等短语的宽泛解释可能会导致谷歌自己的规则出人意料地令人望而却步。</p>
<p class="translated">皮查伊写道:“我们承认这一领域是动态的，不断发展的，我们将带着谦逊的态度对待我们的工作，致力于内部和外部的参与，并愿意随着时间的推移调整我们的方法。”我们将很快看到这种意愿的程度。</p>
			</div>

			</div>    
</body>
</html>