<html>
<head>
<title>Facebook's content moderation system under fire again for child safety failures | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书的内容审核系统因儿童安全问题再次受到抨击</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2017/03/07/facebooks-content-moderation-system-under-fire-for-child-safety-failures/">https://web.archive.org/web/https://techcrunch.com/2017/03/07/facebooks-content-moderation-system-under-fire-for-child-safety-failures/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">在英国广播公司对其报告不当内容的系统进行调查后，脸书再次因未能从其平台上删除儿童剥削图像而受到批评。</p>
<p class="translated">去年，新闻机构报道称，恋童癖者利用脸书的封闭团体来分享儿童受剥削的图片。当时，脸书的公共政策负责人告诉它，他致力于删除“不应该在那里的内容”，脸书后来告诉BBC，它已经改善了其报告系统。</p>
<p class="translated">然而，在今天发表的一篇后续文章中，BBC再次<a target="_blank" href="https://web.archive.org/web/20230404151934/http://www.bbc.com/news/technology-39187929">报道</a>在脸书发现了被分享的儿童色情图片——在BBC最初报道后，这家社交网络巨头未能删除其中的绝大多数。</p>
<p class="translated">英国广播公司表示，它使用脸书报告按钮提醒该公司注意100张似乎违反了其反对淫秽和/或性暗示内容的指导方针的图像——包括来自据称明确针对对儿童有性兴趣的男性的页面。</p>
<p class="translated">据BBC报道，在报道的100张图片中，只有18张被脸书删除。它还发现了五名被定罪的恋童癖者的档案，并通过自己的系统报告给了脸书，但表示没有一个账户被删除——尽管脸书自己的规定禁止被定罪的性犯罪者拥有账户。</p>
<p class="translated">作为对该报告的回应，英国下议院媒体委员会主席Damian Collins告诉<a target="_blank" href="https://web.archive.org/web/20230404151934/http://www.bbc.com/news/technology-39187929"> BBC </a>他对脸书的内容审核系统的有效性有“严重的怀疑”。</p>
<p class="translated">“我认为这提出了一个问题，即用户如何向脸书有效投诉令人不安、不应该出现在网站上的内容，并相信会采取行动，”他说。</p>
<p class="translated">更进一步的是，这家新闻机构在要求发送未被删除的报道内容的例子时，直接与脸书分享了一些报道的图片，随后被脸书向警方举报。</p>
<p class="translated">TechCrunch知道脸书在这一点上遵循了CEOP的指导方针——尽管BBC声称它只是在脸书要求分享报道内容的例子后才发送图片。然而，在英国，观看或分享儿童色情图片是违法的。英国广播公司必须向脸书发送非法内容的链接，而不是直接分享图像以避免被报道——所以故事的这一方面可以归结为一个误解。</p>
<p class="translated">脸书拒绝回答我们的问题——并拒绝接受BBC旗舰新闻节目关于其内容审核问题的采访——但在一份电子邮件声明中，英国政策主任西蒙·米尔纳说:“我们仔细审查了提交给我们的内容，现在已经删除了所有非法或违反我们标准的内容。这个内容已经不在我们的平台上了。我们非常重视此事，并将继续改进我们的报告和记录措施。脸书被公认为互联网上保护儿童安全的最佳平台之一。”</p>
<p class="translated">“任何人传播剥削儿童的图像都是违法的。当英国广播公司给我们发来这样的图片时，我们遵循了我们行业的标准做法，并将它们报告给了CEOP。我们还报道了在我们自己的平台上分享的儿童剥削图片。这件事现在掌握在当局手中，”他补充说。</p>
<p class="translated">这里更大的问题是，脸书的内容审核系统显然还远非完美。上下文内容的调节显然是一个巨大的问题，需要脸书投入更多的资源。即使该公司雇佣了“数千名”人类版主，分布在世界各地的办公室(如都柏林的欧洲内容)，以确保24/7的可用性，但对于一个有超过10亿活跃用户持续分享多种类型内容的平台来说，这仍然是沧海一粟。</p>
<p class="translated">技术解决方案可以是解决方案的一部分——例如<a target="_blank" href="https://web.archive.org/web/20230404151934/https://www.microsoft.com/en-us/PhotoDNA">微软的PhotoDNA云服务</a>，它可以识别已知的虐待儿童图像——但这种系统无法帮助识别未知的淫秽材料。这是一个需要人工调节和足够的人工调节者来及时审查用户报告的问题，以便可以准确地识别问题内容并及时删除——换句话说，与在这种情况下出现的情况相反。</p>
<p class="translated">不能指责脸书领导层对其内容审核失败的担忧视而不见。事实上，首席执行官马克·扎克伯格最近在一封公开信中讨论了这个问题——承认公司需要“做得更多”。他还谈到，他希望技术能够在未来解决这个问题方面发挥更大的作用，认为“人工智能可以帮助提供更好的方法”，并表示脸书正在研究人工智能内容标记系统，以应对不断增长的挑战——尽管他也警告说，这些将需要“许多年才能完全开发出来”。</p>
<p class="translated">简而言之，这就是问题所在。脸书没有投入必要的资源来解决当前的温和问题——即使它将资源用于尝试提出未来可能的解决方案，在那里人工智能温和可以大规模部署。但如果扎克伯格现在想做得更多，简单的解决办法就是雇佣更多的人来审查报告并采取行动。</p>
			</div>

			</div>    
</body>
</html>