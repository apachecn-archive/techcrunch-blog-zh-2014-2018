<html>
<head>
<title>Keeping artificial intelligence accountable to humans | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让人工智能对人类负责</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/08/20/keeping-artificial-intelligence-accountable-to-humans/">https://web.archive.org/web/https://techcrunch.com/2018/08/20/keeping-artificial-intelligence-accountable-to-humans/</a></blockquote><div><div class="article-content">
				<p class="translated">当我十几岁在尼日利亚时，我试图建立一个人工智能系统。激励该领域先驱的同一个梦想激励了我:我们可以创造一种纯逻辑和客观的智能，将人类从人类错误和人类弱点中解放出来。</p>
<p class="translated">我当时在弱计算机系统和断断续续的电力下工作，不用说我的人工智能项目失败了。<span data-term="goog_335853115">十八年后</span>——作为一名研究人工智能、隐私和机器学习算法的工程师——我看到，到目前为止，人工智能可以让我们摆脱主观性或偏见的前提也令人失望。我们正在按照自己的形象创造<a href="https://web.archive.org/web/20230306043025/https://www.rand.org/pubs/research_reports/RR1744.html" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://www.rand.org/pubs/research_reports/RR1744.html&amp;source=gmail&amp;ust=1534868742428000&amp;usg=AFQjCNGpZUmcKFgNqLaS8lolT9tjOagSww">智能</a>。这不是赞美。</p>
<p class="translated">研究人员早就知道，据称中立的算法可以反映甚至强调隐藏在他们获得的数据中的种族、性别和其他偏见。人们发现，在互联网上搜索经常被认为是属于黑人的名字，会促使搜索引擎产生保释代理人的广告。用于求职的算法更有可能向男性求职者推荐比女性更高收入的工作。刑事司法中使用的算法也显示出偏见。</p>
<p class="translated"><span data-term="goog_335853116">五年后</span>，消除算法偏见被证明是一个棘手的问题。仔细梳理数以百万计的子决策，才能找出算法得出结论的原因。即使有可能，也不总是清楚哪个子决策是罪魁祸首。</p>
<p class="translated">然而，这些强大技术的应用发展速度超过了解决缺陷的速度。</p>
<p class="translated">最近的研究强调了这种机器偏见，表明商业面部识别系统在识别浅肤色男性方面表现出色，错误率不到 1%。但是如果你是深色皮肤的女性，你被认错的几率会上升到 35%。</p>
<p class="translated">人工智能系统通常只和用来训练它们的数据一样智能，一样公平。他们使用他们得到的数据中的模式，并一致地应用它们来做出未来的决策。考虑一个人工智能的任务是为医院挑选最好的护士。如果人工智能获得了历史数据——主要是女性的优秀护士的资料——它会倾向于判断女性候选人更适合。算法需要仔细设计，以考虑历史偏见。</p>
<p id="speakable-summary" class="translated"><img decoding="async" class="alignnone size-full wp-image-1432415" src="../Images/8ce4ccabbbe495f0a52e7a5635db16f5.png" alt="" srcset="https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png 750w, https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png?resize=150,106 150w, https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png?resize=300,212 300w, https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png?resize=680,481 680w, https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png?resize=50,35 50w" sizes="(max-width: 750px) 100vw, 750px" data-original-src="https://web.archive.org/web/20230306043025im_/https://techcrunch.com/wp-content/uploads/2016/12/facebook_facial_rec.png"/></p>
<p class="translated">偶尔，人工智能系统会食物中毒。最著名的案例是<a href="https://web.archive.org/web/20230306043025/https://www.inverse.com/article/13630-what-has-ibm-watson-been-up-to-since-winning-jeopardy-5-years-ago" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://www.inverse.com/article/13630-what-has-ibm-watson-been-up-to-since-winning-jeopardy-5-years-ago&amp;source=gmail&amp;ust=1534868742428000&amp;usg=AFQjCNHkR-tqA4j0q9utbfIFpK_U_XgA3w">沃森</a>，2011 年在电视游戏节目《危险边缘》中首次击败人类的人工智能。IBM 的沃森主人需要教它语言，包括美国俚语，所以他们给了它在线城市词典的内容。但在吃了那顿丰富多彩的语言餐后，沃森养成了骂人的习惯。它开始用四个字母的单词来标点它的回答。</p>
<p class="translated">我们必须小心输入算法的内容。公司现在终于明白，他们无法通过主要使用白人男性的照片来训练面部识别技术。但是仅仅更好的训练数据并不能解决让算法实现公平的根本问题。</p>
<p class="translated">算法已经可以告诉你你可能想读什么，你可能想和谁约会，你可能在哪里找到工作。当他们能够就谁被雇用、谁获得贷款或刑期长短提出建议时，人工智能将必须变得更加透明——更加负责任，并尊重社会的价值观和规范。</p>
<p class="translated">当人工智能做出敏感决策时，问责制始于人类的监督。微软总裁布拉德·史密斯最近采取了一项不同寻常的举措，呼吁美国政府考虑要求对面部识别技术进行人工监督。</p>
<p class="translated">下一步是披露人类何时受制于人工智能做出的决定。自上而下的政府监管可能不是解决算法偏差的可行或可取的方法。但是可以创造出一些程序，让人们可以通过诉诸人类来对机器做出的决定提出上诉。欧盟新的<a href="https://web.archive.org/web/20230306043025/https://www.cnet.com/how-to/what-gdpr-means-for-facebook-google-the-eu-us-and-you/" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://www.cnet.com/how-to/what-gdpr-means-for-facebook-google-the-eu-us-and-you/&amp;source=gmail&amp;ust=1534868742428000&amp;usg=AFQjCNGVHYzU9LdzwbXLzV3Z2MtvAetJIg">通用数据保护条例</a>确立了个人了解和质疑自动决策的权利。</p>
<p class="translated">如今，无论是在机场还是在就业数据库中，被错误识别的人都没有追索权。他们可能被故意拍下驾照，或者被监控摄像头偷偷拍下(这有更高的错误率)。他们不知道他们的图像存储在哪里，是否已经出售，或者谁可以访问它。他们无法知道自己是否受到了错误数据或不公平决策的伤害。</p>
<p class="translated">这种不成熟的技术已经使少数民族处于不利地位，他们为改善整个社会的安全所承担的负担是不公平的，也是没有得到补偿的。单靠工程师无法解决这个问题。人工智能系统就像一个非常聪明的孩子，刚刚开始理解歧视的复杂性。</p>
<p class="translated">为了实现我十几岁时的梦想，即人工智能能够将人类从偏见中解放出来，而不是加剧偏见，需要一系列专家和监管机构更深入地思考人工智能不仅能做什么，而且应该做什么——然后教会它如何做。<em> </em></p>
			</div>

			</div>    
</body>
</html>