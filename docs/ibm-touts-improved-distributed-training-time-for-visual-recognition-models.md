# IBM 宣称改进了视觉识别模型的分布式训练时间 

> 原文：<https://web.archive.org/web/https://techcrunch.com/2017/08/07/ibm-touts-improved-distributed-training-time-for-visual-recognition-models/>

# IBM 吹嘘改进了视觉识别模型的分布式训练时间

两个月前，脸书的人工智能研究实验室(FAIR)公布了一些令人印象深刻的大规模分布式视觉识别模型的训练时间。今天，IBM 用自己的一些数据进行了反击。IBM 的研究小组表示，它能够在 50 分钟内在 256 个 GPU 上训练 ResNet-50 的 1k 课程——这实际上只是“我的模型比你的模型训练得更快”的礼貌说法脸书指出，使用 Caffe2，[能够使用 8k 小批量方法在 256 个 GPU 上在一小时内训练一个类似的 ResNet-50 模型。](https://web.archive.org/web/20221206153044/https://beta.techcrunch.com/2017/06/08/facebook-is-speeding-up-training-for-visual-recognition-models/)

这将是一个自然的时刻来质疑为什么这些事情会摆在首位。分布式处理是人工智能研究的一个很大的子领域，但它也很神秘。对于深度学习问题来说，计算任务通常非常大，以至于在大量 GPU 上处理它们比在单个 GPU 上处理效率更高。

但是当你添加更多的 GPU 时，训练时间不会自然减少。例如，您可能会假设，如果用一个 GPU 训练需要两分钟，那么用两个 GPU 训练只需要一分钟。在现实世界中，情况并非如此，因为拆分和重组复杂的定量运算需要一些成本。

IBM 承诺的是最有效的分布式深度学习库，用于将一个巨大的深度学习问题分解成数百个更小的深度学习问题。在单个计算工作的环境中，这一切看起来似乎微不足道，但请记住，像 IBM 和脸书这样的公司整天都在为数百万客户训练模型。每一家主要的科技公司都与此有利害关系，但通常很难比较公司承诺的结果，因为任何研究工作都有大量的变量。

现在，你有理由质疑专注于分布式扩展效率增量的未来意义——你是对的。IBM Research 的系统加速和内存主管 Hillery Hunter 告诉我，每个人都非常接近最佳状态。

“你已经尽你所能从这个系统中获得了很多，所以我们相信我们已经接近最佳状态。真正的问题是我们不断看到改进的速度，以及我们是否仍将看到整体学习时间的改进。”

IBM 并没有止步于 ResNet-50 的结果。该公司继续在 ResNet-101 上测试分布式培训，这是一个更大更复杂的视觉识别模型。该团队表示，他们能够在 7 个小时内使用 256 个 GPU 在 ImageNet-22k 数据集上训练 ResNet-101，这对于挑战来说是一个相当令人印象深刻的时间。

“这也有利于在较小系统上运行的人，”亨特补充道。“您不需要 256 个 GPU 和 64 个系统来获得优势。”

深度学习库与主要的开源深度学习框架配合良好，包括 TensorFlow、Caffe 和 Torch。如果你想亲自尝试，一切都可以通过 PowerAI 获得。