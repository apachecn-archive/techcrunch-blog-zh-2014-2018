# 如何(以及如何不)修复人工智能

> 原文：<https://web.archive.org/web/https://techcrunch.com/2018/07/26/how-and-how-not-to-fix-ai/>

约书亚·纽是世界银行的高级政策分析师

[Center for Data Innovation](https://web.archive.org/web/20230306043016/http://www.datainnovation.org/)

，一个研究数据、技术和公共政策交叉的智库。

虽然人工智能曾被誉为开启经济繁荣新时代的关键，但政策制定者今天面临着一波要求确保人工智能公平、道德和安全的呼声。纽约市长白思豪(de Blasio)最近宣布成立[国家第一个工作组](https://web.archive.org/web/20230306043016/http://www1.nyc.gov/office-of-the-mayor/news/251-18/mayor-de-blasio-first-in-nation-task-force-examine-automated-decision-systems-used-by)来监控和评估算法的使用。几天后，欧盟颁布了全面的新数据保护规则，要求公司能够向消费者解释任何[自动决策](https://web.archive.org/web/20230306043016/http://www2.datainnovation.org/2018-impact-gdpr-ai.pdf)。像[埃隆·马斯克](https://web.archive.org/web/20230306043016/https://www.c-span.org/video/?431119-6/elon-musk-addresses-nga)这样的高调批评者呼吁政策制定者采取更多措施监管人工智能。

不幸的是，两个最受欢迎的想法——要求公司公开其算法的源代码，并解释它们如何做出决策——通过监管使用人工智能的公司的商业模式和算法的内部工作，而不是让这些公司对结果负责，将弊大于利。

第一个想法——“算法透明”——要求公司披露其人工智能系统中使用的源代码和数据。除了简单之外，作为一个大规模的解决方案，这个想法缺乏任何真正的优点。许多人工智能系统过于复杂，仅靠查看源代码无法完全理解。一些人工智能系统依赖于数百万个数据点和数千行代码，当遇到新数据时，决策模型会随着时间而改变。当该系统的开发者[可能也无法发现所有潜在的违法行为时，期望即使是最有动力、资源最丰富的监管者或相关公民也能够发现所有潜在的违法行为是不现实的。](https://web.archive.org/web/20230306043016/https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)

此外，并不是所有的公司都有开源商业模式。要求他们公开源代码降低了他们投资开发新算法的动力，因为这会吸引竞争对手抄袭。在[中国](https://web.archive.org/web/20230306043016/https://www.axios.com/china-broke-hacking-pact-before-new-tariff-tiff-d19f5604-f9ce-458a-a50a-2f906c8f12ab.html)，与美国激烈竞争人工智能的统治地位，但经常无视知识产权的坏人，可能会利用透明度要求来窃取源代码。

另一个想法——“算法的可解释性”——要求公司向消费者解释他们的算法是如何做出决策的。这个提议的问题是，在人工智能系统中，可解释性和准确性之间往往存在不可避免的权衡。算法的准确性通常与其复杂性成比例，因此算法越复杂，解释起来就越困难。虽然随着对可解释人工智能的研究日益成熟，这种情况可能会在未来发生变化——DARPA 在 2017 年为这个问题投入了 7500 万美元()——但就目前而言，对可解释性的要求将以准确性为代价。这是非常危险的。例如，对于自动驾驶汽车，解释事故更重要还是避免事故更重要？解释比准确性更重要的情况很少见。

> 关于如何让人工智能变得安全的辩论忽略了对细致入微、有针对性的监管方法的需求。

政策制定者不应该要求公司披露他们的源代码或限制他们可以使用的算法类型，而是应该坚持[算法问责制](https://web.archive.org/web/20230306043016/http://www.datainnovation.org/2018/05/how-policymakers-can-foster-algorithmic-accountability/)——算法系统应该采用各种控制措施来确保操作者(即负责部署算法的一方)可以验证它的行为符合预期，并在出现有害结果时识别和纠正有害结果。

围绕算法问责建立的政策框架有几个重要的好处。首先，它会让运营商对他们的算法可能造成的任何伤害负责，而不是开发者。运营商不仅对算法如何影响社会具有最大的影响力，而且他们已经必须遵守各种法律，以确保他们的决定不会造成伤害。例如，雇主在招聘时必须遵守反歧视法，不管他们是否使用算法来做出这些决定。

第二，让操作者对结果负责，而不是对算法的内部工作负责，将使他们能够专注于确保其算法不会造成伤害的最佳方法，如在适当的情况下，进行置信度测量、影响评估或程序规则性。例如，大学可以在部署人工智能系统之前进行影响评估，该系统旨在预测哪些学生可能辍学，以确保其有效和公平。与透明度或可解释性要求不同，这将使大学能够有效地识别任何潜在的缺陷，而不禁止使用复杂的专有算法。

这并不是说透明度和解释没有它们的位置。例如，透明度要求对于刑事司法系统中的[风险评估算法是有意义的。毕竟，要求司法系统接受尽可能最高程度的审查符合](https://web.archive.org/web/20230306043016/https://www.rstreet.org/2017/12/01/when-it-comes-to-criminal-justice-ai-we-need-transparency-and-accountability/)[的长期公共利益](https://web.archive.org/web/20230306043016/https://www.rstreet.org/2017/12/01/when-it-comes-to-criminal-justice-ai-we-need-transparency-and-accountability/)，即使这种透明度可能无法揭示先进的机器学习系统是如何工作的。

类似地，像《平等信贷机会法案》这样的法律要求公司向消费者提供拒绝他们信贷的充分解释[。无论一家公司是否使用人工智能来做决策，消费者仍然有权获得这些解释。](https://web.archive.org/web/20230306043016/https://www.legalmatch.com/law-library/article/rights-if-denied-credit.html?intakeredesigned=1)

关于如何让人工智能变得安全的辩论忽略了对细致入微、有针对性的监管方法的需求，将算法的透明度和可解释性视为银弹，而没有考虑它们的许多缺点。想要减轻人工智能造成的潜在危害没有错，但迄今为止提出的过于简单、过于宽泛的解决方案在很大程度上是无效的，并且可能弊大于利。算法问责为确保组织负责任地使用人工智能提供了更好的途径，以便它能够真正造福社会。