<html>
<head>
<title>5 unexpected sources of bias in artificial intelligence | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中 5 个意想不到的偏见来源</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2016/12/10/5-unexpected-sources-of-bias-in-artificial-intelligence/">https://web.archive.org/web/https://techcrunch.com/2016/12/10/5-unexpected-sources-of-bias-in-artificial-intelligence/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary">
</p><div class="article__contributor-byline">
	<div class="contributor-byline__contributor">
		<p class="byline__author translated"><span class="byline__author-name">克里斯蒂安·哈蒙德</span><span class="byline__author-title">撰稿人</span></p>

			</div>

		<div class="contributor-byline__bio"><p class="translated">作为首席科学家和联合创始人，克里斯专注于 R&amp;D 在</p><a href="https://web.archive.org/web/20230227172708/https://www.narrativescience.com/">Narrative Science</a><p class="translated">。克里斯也是西北大学的计算机科学教授。</p></div>
	
	</div>

<p class="translated">我们倾向于认为机器，尤其是智能机器，在某种程度上是冷酷的，有计算能力的，没有偏见的。我们认为，在驾驶员和随机行人之间的生死抉择中，自动驾驶汽车不会有任何偏好。我们相信，执行信用评估的智能系统将忽略一切，除了真正有影响力的指标，如收入和 FICO 分数。我们知道，学习系统将总是收敛于基本事实，因为无偏的算法驱动它们。</p>
<p class="translated">对于我们中的一些人来说，这是一个错误:机器不应该在它们僵化的观点之外具有同理心。对其他人来说，这是一个特征:他们应该摆脱人类的偏见。但在中间，有一种观点认为他们会是客观的。</p>
<p class="translated">当然，这与事实相去甚远。现实是，不仅很少有智能系统是真正无偏见的，而且有多种偏见来源。这些来源包括我们用来训练系统的数据，我们在“野生”中与它们的交互，突发偏差，相似性偏差和冲突目标的偏差。这些来源大多不为人知。但是，当我们构建和部署智能系统时，理解它们是至关重要的，这样我们就可以有意识地进行设计，并有希望避免潜在的问题。</p>
<h2 class="translated">数据驱动偏差</h2>
<p class="translated">对于任何学习的系统来说，输出是由它接收的数据决定的。这并不是一个新的见解，当我们看到由数百万个例子驱动的系统时，它往往会被遗忘。人们的想法是，大量的例子将压倒任何人类的偏见。但是如果训练集本身是有偏差的，结果也是一样的。</p>
<p class="translated">最近，这种偏见已经在通过深度学习进行图像识别的系统中出现。尼康对<a target="_blank" href="https://web.archive.org/web/20230227172708/http://gizmodo.com/5256650/camera-misses-the-mark-on-racial-sensitivity">亚洲面孔的困惑</a>和惠普人脸识别软件中的<a target="_blank" href="https://web.archive.org/web/20230227172708/https://www.wired.com/2009/12/hp-notebooks-racist/">肤色问题</a>似乎都是从扭曲的样本集中学习的产物。虽然这两个问题都是可以解决的，而且绝对不是有意的，但它们表明了当我们不注意数据中的偏差时可能出现的问题。</p>
<p class="translated">除了面部识别，现实世界中还有其他令人不安的例子。学习系统用于建立规则集，这些规则集用于预测假释犯、犯罪模式或潜在雇员的<a target="_blank" href="https://web.archive.org/web/20230227172708/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">累犯率</a>是具有潜在负面影响的领域。当他们使用有偏差的数据，甚至是平衡的数据进行训练，但系统在决策中有偏见时，他们也会使偏见永久化。</p>
<h2 class="translated">通过互动产生偏见</h2>
<p class="translated">一些系统通过大量查看一组例子来学习，其他种类的系统通过交互来学习。偏见是基于驱动交互的用户的偏见而产生的。这种偏见的一个明显例子是<a target="_blank" href="https://web.archive.org/web/20230227172708/http://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">微软的 Tay </a>，这是一个基于 Twitter 的聊天机器人，旨在从与用户的互动中学习。不幸的是，Tay 受到了一个用户社区的影响，这个社区教导 Tay 是种族主义者和厌恶女性者。本质上，社区在 Tay 上反复发布攻击性的言论，系统将这些言论用作以后回应的素材。</p>
<p class="translated">Tay 仅仅活了 24 小时，就被微软关闭了，因为它已经变成了一个相当激进的种族主义者。虽然泰的种族主义咆哮仅限于 Twitter 领域，但它表明了潜在的现实世界的影响。当我们建造智能系统，与人类伙伴一起做决策并向他们学习时，同样的糟糕训练问题可能会出现在更有问题的环境中。</p>
<p class="translated">相反，如果我们让智能系统与能够长期指导它们的人合作，会怎么样？想想我们对决定谁能获得贷款甚至谁能获得假释的机器的不信任。Tay 教给我们的是，这种系统将学习周围环境和人的偏见，无论是好是坏，反映训练它们的人的意见。</p>
<h2 class="translated">紧急偏差</h2>
<p class="translated">有时，由系统做出的旨在个性化的决定最终会在我们周围产生偏见“泡沫”。我们只需看看脸书的现状，就能看出这种偏见在起作用。在顶层，脸书用户可以看到他们朋友的帖子，并可以与他们分享信息。</p>
<p class="translated">不幸的是，任何使用数据馈送分析来呈现其他内容的算法都将提供与用户已经看到的想法集相匹配的内容。随着用户打开、喜欢和分享内容，这种效应会被放大。结果是信息流偏向用户现有的信念集。</p>
<p class="translated">虽然它肯定是个性化的，并且经常令人放心，但它不再是我们倾向于认为的新闻。这是一个信息泡沫，是“确认偏差”的算法版本。用户不必屏蔽与他们的信念相冲突的信息，因为系统会自动为他们做这件事。</p>
<p>	</p><div class="article-block block--pullout block--right">
		<blockquote class="translated">在理想世界中，智能系统及其算法应该是客观的。</blockquote>
	</div>
	
<p class="translated">这些信息偏见对新闻世界的影响令人担忧。但是，当我们将社交媒体模型视为支持企业决策的一种方式时，支持信息泡沫出现的系统有可能扭曲我们的思维。一个只从与他或她想法相同的人那里获取信息的知识工作者，永远不会看到相反的观点，并且会倾向于忽视和否定其他选择。</p>
<h2 class="translated">相似性偏差</h2>
<p class="translated">有时候，偏见只是系统做了它们被设计要做的事情的产物。例如，谷歌新闻旨在提供将用户查询与一组相关报道相匹配的报道。这显然是它的设计目的，而且它做得很好。当然，结果是一组相似的故事，往往相互证实和确证。也就是说，他们定义了一个信息泡沫，类似于与脸书相关的个性化泡沫。</p>
<p class="translated">这种模式肯定突出了与新闻的作用及其传播有关的问题——最明显的一个问题是对信息的平衡处理。缺乏“编辑控制”在很多情况下都会出现。虽然相似性是信息世界中一个强大的度量标准，但绝不是唯一的。不同的观点为决策提供了有力的支持。只提供与查询或现有文档“相似”的结果的信息系统会产生自己的泡沫。</p>
<p class="translated">相似性偏见是一种倾向于被接受的偏见，即使收缩、反对甚至冲突的观点支持创新和创造，特别是在企业中。</p>
<h2 class="translated">冲突的目标偏见</h2>
<p class="translated">有时候，为非常具体的业务目的而设计的系统最终会出现真实但完全不可预见的偏差。</p>
<p class="translated">例如，想象一个系统，它被设计为向潜在的候选人提供工作描述。当用户点击职位描述时，系统产生收入。因此，该算法的目标自然是提供点击率最高的职位描述。</p>
<p/>
<p class="translated">事实证明，人们倾向于点击符合他们自我观点的工作，而这种观点可以通过简单地呈现出来而朝着刻板印象的方向得到加强。例如，女性面对的工作标签是“护理”而不是“医疗技师”，她们会倾向于前者。不是因为这些工作最适合他们，而是因为他们会被提醒这种刻板印象，然后与它保持一致。</p>
<p class="translated"><a target="_blank" href="https://web.archive.org/web/20230227172708/http://fulltext.study/preview/pdf/1019473.pdf">刻板印象威胁</a>对行为的影响是这样的，符合个人对与其相关的刻板印象的知识(如性别、种族、民族)的工作展示会导致更多的点击。因此，任何具有基于点击行为的学习组件的网站都会倾向于提供强化刻板印象的机会。</p>
<h2 class="translated">机器偏见就是人类偏见</h2>
<p class="translated">在理想世界中，智能系统及其算法应该是客观的。不幸的是，这些系统是我们自己建立的，因此最终反映了我们的偏见。通过理解偏差本身和问题的来源，我们可以积极地设计系统来避免它们。</p>
<p class="translated">也许我们永远也不可能创造出完全客观的系统和工具，但至少它们会比我们更少偏见。然后，也许<a target="_blank" href="https://web.archive.org/web/20230227172708/https://www.cnet.com/news/facebook-mark-zuckerberg-fake-news-affect-election-techonomy-donald-trump-crazy/">选举不会暗算</a>美国，<a target="_blank" href="https://web.archive.org/web/20230227172708/https://www.technologyreview.com/s/602586/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">货币不会崩溃</a>，我们会发现自己在与我们<a target="_blank" href="https://web.archive.org/web/20230227172708/http://www.mediapost.com/publications/article/289099/mobs-filter-bubbles-democracy.html">个性化新闻气泡</a>之外的人交流。</p>
			</div>

			</div>    
</body>
</html>