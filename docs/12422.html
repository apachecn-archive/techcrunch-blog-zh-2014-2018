<html>
<head>
<title>Algorithmic accountability | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">算法问责| TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2017/04/30/algorithmic-accountability/">https://web.archive.org/web/https://techcrunch.com/2017/04/30/algorithmic-accountability/</a></blockquote><div><div class="article-content">
				<p class="first translated">当网飞建议你在看完《爱情》后看《格雷斯和弗兰基》时，一个算法决定这将是你下一个要看的合乎逻辑的东西。当谷歌把一个搜索结果显示在另一个之前时，一种算法会做出一个决定，认为一个页面比另一个页面更重要。哦，当一个照片应用程序决定你肤色浅会更好看时，<a href="https://web.archive.org/web/20230406160730/https://techcrunch.com/2017/04/25/faceapp-apologises-for-building-a-racist-ai/" target="_blank" rel="noopener noreferrer">一个真人开发的严重有偏见的算法</a>做出了这个决定。</p>
<p id="speakable-summary" class="translated">算法是计算机为了解决问题并对特定的行动过程做出决策而遵循的一套规则。无论是我们接收到的信息类型，人们看到的关于我们的信息，我们受雇做的工作，我们获得批准的信用卡，以及未来看到我们或看不到我们的无人驾驶汽车，算法正日益成为我们生活的一大部分。</p>
<p class="translated">但算法有一个固有的问题，它从最基础的层面开始，并在整个适应过程中持续存在:这些基于机器的决策者中存在人类偏见。</p>
<p class="translated">你可能还记得那次<a href="https://web.archive.org/web/20230406160730/https://www.nytimes.com/2017/02/24/technology/anthony-levandowski-waymo-uber-google-lawsuit.html" target="_blank" rel="noopener noreferrer">优步的无人驾驶汽车在三藩市闯红灯</a>，或者<a href="https://web.archive.org/web/20230406160730/http://blogs.wsj.com/digits/2015/07/01/google-mistakenly-tags-black-people-as-gorillas-showing-limits-of-algorithms/" target="_blank" rel="noopener noreferrer">谷歌的照片应用程序将黑人的图像标记为大猩猩</a>。马萨诸塞州机动车登记处的面部识别算法错误地将某人标记为罪犯，并吊销了他们的驾照。而<a href="https://web.archive.org/web/20230406160730/https://motherboard.vice.com/en_us/article/microsoft-suspends-ai-chatbot-after-it-veers-into-white-supremacy-tay-and-you" target="_blank" rel="noopener noreferrer">微软的 bot Tay 变得无赖</a>决定成为白人至上主义者。这些是最坏的算法。最近，谷歌搜索结果和脸书出现的假新闻事件也让他们成为关注的焦点。</p>
<p class="translated">但是算法失控会产生更大的影响；它们会给不知情的人带来改变生活的后果。想想有算法偏见的自动驾驶汽车、无人机和其他种类的自动车辆会有多可怕。考虑在算法上对黑人有偏见或不能正确识别非白人的机器人，然后在此人不是人类的基础上做出决定。</p>
<p class="translated">另一个需要考虑的重要因素是算法在决定我们眼中的世界以及人们如何看待我们方面的作用。想想由算法“驾驶”的无人驾驶汽车撞倒黑人，因为它们不承认黑人是人类。或者<a href="https://web.archive.org/web/20230406160730/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank" rel="noopener noreferrer">预测未来罪犯的算法软件</a>，恰好对黑人有偏见。</p>
<p class="translated">坏的或错误的数据、好的但有偏见的数据(因为没有足够的数据)或不灵活的模型(无法解释不同的情况)都可能导致各种问题。</p>
<p class="translated">难题是弄清楚如何处理这些有问题的算法结果。许多研究人员和学者都在积极探索如何提高算法的可问责性。如果科技公司提供他们的代码以使这些算法决策更加透明，这将意味着什么？此外，如果某种类型的政府委员会负责审查它们，会发生什么情况？</p>
<p class="first translated">美国前首席数据科学家 DJ Patil 告诉我，无论采取什么方法来确保从算法的开发中消除偏见，都不会严重阻碍进步。他补充道，只有科技公司充分认识到自己在维持和延续偏见、歧视和谎言方面的角色，解决方案才能得以实施，因此才会有效。</p>
<p class="translated">他补充道:如果你将我们几年前面临的问题与我们现在面临的问题进行对比，就会发现它们正在复合。“那么，我们如何应对这一挑战？”在开发新技术时，这些算法背后的团队需要更加多样化。帕蒂尔说，这是不可否认的，但问题是整个多元化领域的可扩展性。</p>
<p class="translated">“有种族的多样性，有宗教的多样性，有残疾的多样性。我见过多少次有人设计一些东西，如果他们做了一个轻微的决定选择，你就可以为更广泛的社会设计——很容易，只是做了一个轻微的设计改变，但他们不知道。我认为挑战之一是我们没有可扩展的模板来做到这一点。”</p>
<p class="translated">例如，谷歌决定了许多人在互联网上看到的内容。正如弗兰克·帕斯夸尔在他的书《黑箱社会:控制金钱和信息的秘密算法》中所写的那样，谷歌和其他科技公司设定了评判我们所有人的标准，但没有人真正评判他们。</p>
<p class="translated">当你用谷歌图片搜索“人”时，你看不到多少有色人种，这就延续了白人的正常化，并重新激起了种族偏见。相反，你会看到很多白人男性的照片，<a href="https://web.archive.org/web/20230406160730/https://datasociety.net/" target="_blank" rel="noopener noreferrer">数据&amp;社会研究中心</a>的员工、曾在 X 和搜索基础设施部门工作的前谷歌员工索雷尔·弗里德勒说。</p>
<p class="translated">“不幸的是，这也许代表了‘人’在我们社会中被广泛使用的方式，”弗里德勒说。“那么接下来的问题是，这种语言表达方式适合用于图像搜索吗？然后谷歌需要决定，如果你专门搜索黑人，我可以接受只显示黑人吗？我认为这是一个关于我们希望我们的社会是什么样子的哲学决定，我认为这是一个值得考虑的问题。”</p>
<p class="translated">也许谷歌并不认为自己有很大的责任去干预这种情况。也许在这种情况下，“这是我们的用户输入的结果”的说法是可以接受的。但与大屠杀或自杀相关的搜索查询促使谷歌介入。</p>
<p class="translated">算法决定了谷歌的搜索结果和建议。谷歌的一些算法失误比其他失误更令人震惊，有时谷歌会介入，但通常不会。</p>
<p class="translated">“如果你寻找自杀的方法，你会被引向自杀热线，”数据与社会的研究分析师罗宾·卡普兰告诉 TechCrunch。“谷歌认为有些事情与公众利益相关，他们愿意进行某种程度的干预和防范，但对于他们如何评估这些事情，真的没有很好的理解。”</p>
<p class="translated">今年早些时候，如果你搜索类似“<a href="https://web.archive.org/web/20230406160730/https://mic.com/articles/165038/did-the-holocaust-happen-google-s-top-search-results-still-say-it-s-a-hoax#.fyhl3CiCG" target="_blank" rel="noopener noreferrer">大屠杀是真的吗？、“大屠杀发生了吗</a>”或者“<a href="https://web.archive.org/web/20230406160730/https://www.indy100.com/article/are-black-people-smart-google-result-7487181" target="_blank" rel="noopener noreferrer">黑人聪明吗？</a>“两个查询的第一个搜索结果中有一个非常有问题。直到人们表示愤怒，谷歌<a href="https://web.archive.org/web/20230406160730/http://searchengineland.com/google-holocaust-denial-site-gone-266353" target="_blank" rel="noopener noreferrer">才决定做点什么</a>。</p>
<p class="translated"><img decoding="async" class="breakout wow" src="../Images/fd487f4f87f7cad6e3ccca00948f271a.png" data-original-src="https://web.archive.org/web/20230406160730im_/https://techcrunch.com/wp-content/uploads/2017/04/google-algorithm-search.png?w=1024"/></p>
<p class="translated">“当非权威信息在我们的搜索结果中排名过高时，我们会开发可扩展的自动化方法来解决问题，而不是手动逐个删除这些信息，”谷歌发言人通过电子邮件告诉 TechCrunch。“我们正在努力改进我们的算法，这将有助于在网络上呈现更多高质量、可信的内容，我们将随着时间的推移继续改进我们的算法，以应对这些挑战。”</p>
<p class="translated">除了搜索结果和建议，谷歌的照片算法和广告也存在问题。谷歌的照片算法一度错误地将黑人标记为大猩猩。</p>
<p class="translated">谷歌在 2015 年 5 月推出了 Photos，接收效果相对较好。但是在开发者杰基·阿尔辛指出这个缺陷后，当时负责谷歌照片的布拉德利·霍洛维茨说他的收件箱着火了。</p>
<p class="translated">“那天是我职业生涯中最糟糕的一天，也许是我一生中最糟糕的一天，”Horowitz 在 12 月份说道。</p>
<p class="translated">Horowitz 说:“人们在 gorilla 中输入，非裔美国人在搜索结果中被返回。他说，这是如何发生的，因为有垃圾进来，也有垃圾出去——他说这在计算机科学中很常见。“在某种程度上，数据是性别歧视或种族主义的，你会让算法模仿这些行为。”</p>
<p class="translated">霍洛维茨补充说，谷歌的员工基础并不代表它所服务的用户。他承认，如果谷歌有一个更多元化的团队，该公司会在开发过程中更早注意到这些问题。</p>
<p class="translated">还有一次，谷歌在名字听起来“像黑人”的人的搜索结果顶部显示了面部照片。Latanya Sweeney 是哈佛大学政府和技术方面的黑人教授，也是数据隐私实验室的创始人，她在 2013 年发表了她对 Google AdWords 的研究，引起了公众的注意。她发现，当人们在谷歌上搜索传统上属于黑人的名字时，显示的广告是逮捕记录和面部照片。</p>
<p class="translated">帕斯奎尔说，导致这种错误的原因是，人们认为自然界和自然过程就像人类的社会世界和社会过程一样。</p>
<p class="translated">“这是一种假设，如果我们能开发出一种算法，正确地挑出所有的岩石，我们就能有一种正确地或以一种有用的方式或类似的方式对人进行分类，”Pasquale 说。“我认为这是根本问题。他们采用了许多自然科学方法，并把它们投入到社会情境中，他们没有试图调整干预措施来反映人类价值观。”</p>
<p class="translated">当一个算法产生不太理想的结果时，可能是数据集一开始就不好，算法不够灵活，产品背后的团队没有充分考虑用例，人类与算法的交互不足以操纵它，甚至以上所有情况。帕蒂尔说，但科技公司再也不会说，‘哦，这只是一个应用程序’或‘哦，我们没有数据。当你在设计一个真正影响人们生活的产品时，我们有不同层次的责任。"</p>
<p class="translated">虽然算法也有改变我们世界的巨大潜力，但谷歌的上述失败表明了一个更大的问题:算法在维持或延续歧视和偏见的历史模式或传播虚假信息方面的作用。</p>
<p class="translated">帕蒂尔说:“数据既有可能造成伤害，也有可能带来巨大的帮助。”。“我们经常关注有害的一面，人们谈论数学的方式——我们应该害怕它，以及为什么我们应该如此害怕它。</p>
<p class="translated">“我们必须记住这些算法，这些技术将成为我们解决癌症的方法。这就是我们如何治愈下一种疾病的方法。这是我们抗击埃博拉和寨卡等危机的方式。大数据是解决方案。”</p>
<p class="first translated">解决与互联网内容相关的算法问题的一个障碍是《通信规范法案》第 230 条，该条款规定，“交互式计算机服务的任何提供商或用户都不应被视为另一信息内容提供商所提供的任何信息的发布者或发言人。”</p>
<p class="translated">它让科技公司有可能扩大规模，因为它解除了平台处理用户非法或令人反感行为的任何责任。电子前沿基金会称其为“保护互联网上言论自由和创新的最有价值的工具之一。”</p>
<p class="translated">如果这项法律不存在，我们基本上可以授权谷歌和其他科技公司对我们认为不良的言论进行审查。类似的事情正在欧洲发生，T2 有被遗忘的权利。</p>
<p class="translated">“美国学者和政策制定者有些害怕，因为基本上在那里发生的事情实际上来说，谷歌已经成为这些主张的仲裁者，”Solon Barocas 说，他是微软研究院 T4 纽约实验室的博士后研究员，也是社会、伦理和人工智能小组的成员。“这并不像是一个政府机构在管理应该撤销什么的决定。相反，它说‘谷歌你有责任这么做’，然后谷歌自己就这么做了。这吓坏了很多美国人。”</p>
<p class="translated">但鉴于 CDA 第 230 条的存在以及它为平台提供许多保护的事实，可能很难在美国使用立法手段来影响哪些内容在脸书流行或哪些搜索结果出现在谷歌上。</p>
<p class="translated">然而，卡普兰说，在美国以外，立法可能会影响这些科技公司在美国境内的运营方式。例如，在德国，政府起草了一项法律，对未能删除假新闻或仇恨言论的社交网络处以高达 5000 万欧元的罚款。</p>
<p class="translated">与此同时，今年早些时候，欧盟数字主管安德鲁斯·安西普警告脸书说，虽然他相信自律措施，但如果必要的话，他准备采取立法行动。</p>
<p class="translated">卡普兰说:“我们过去看到的是，发生在美国以外的这类政策确实在塑造美国信息结构方面发挥了相当大的作用。”。“所以，如果你看看谷歌的自动完成算法，你会看到类似的事情——不同的自动完成是不允许的，因为诽谤案件发生在国外，即使谷歌在这里受到保护。那些清楚了解自己愿意监管媒体的国家提出的政策可能会产生有趣的影响。”</p>
<p class="translated">即使第 230 条仍然存在，它很可能会存在，也有办法重新评估和重新编程算法，以做出更好的决定，并在潜在的偏见或歧视性结果发生之前规避它们。</p>
<p class="first translated">虽然开发软件的团队需要更加多样化，以便真正考虑到算法可能需要处理的不同场景，但对于每个公司的算法问题，没有简单明了的现成解决方案。但是研究人员提出了几种解决算法责任的潜在方法。</p>
<p class="translated">巴罗卡斯告诉我，快速发展的两个领域分别与前端和后端过程有关。前端方法包括确保某些价值在科技公司建立的算法模型中得到编码和实现。例如，科技公司可以确保对歧视和公平的担忧是算法过程的一部分。</p>
<p class="translated">“确保有某些公平的想法来约束模型的行为，并且可以提前完成，这意味着在开发该程序的过程中，您可以确保这些事情得到满足。”</p>
<p class="translated">在后端，您可以想象开发人员在不完全确定系统将如何运行的情况下构建和部署系统，并且无法预测它们可能产生的潜在不利结果。巴罗卡斯说，你要做的是建立一个系统，给它输入一堆例子，然后观察它的行为。</p>
<p class="translated">假设这个系统是一辆自动驾驶汽车，你向它提供行人的例子(比如一个白人对一个黑人对一个残疾人)。通过基于各种输入/示例分析系统如何运行，可以看出该过程是否具有歧视性。如果汽车只为白人停车，但决定撞黑人和残疾人，显然算法有问题。</p>
<p class="translated">巴罗卡斯说:“如果你做得足够多，你就可以梳理出结果中是否存在任何类型的系统性偏差或系统性差异，这也是人们正在做大量工作的领域。”"这就是众所周知的算法审计."</p>
<p class="translated">Pasquale 说，当人们谈论算法责任时，他们通常是在谈论算法审计，其中有三个不同的层次。</p>
<p class="translated">“就算法问责制而言，第一步是数据和算法的透明度，”Pasquale 说。“在数据方面，我们可以做得更多，以确保透明度，比如说，我的脸书供稿或谷歌搜索结果中包含了哪些信息。”</p>
<p class="translated">例如，让人们更好地了解他们的脸书新闻源、谷歌搜索结果和建议以及推特源的内容。</p>
<p class="translated">“第一步是让他们确切地了解他们所拥有的全部数据，”Pasquale 说。</p>
<p class="translated">下一步是 Pasquale 所谓的合格透明度，外界的人来检查，看看是否有什么不顺心的事情发生。帕斯奎尔说，最后一部分，也可能是最困难的一部分，是让科技公司“为他们所做的歧视性影响承担某种道德和社会责任”。</p>
<p class="translated">帕斯奎尔说，算法问责的根本障碍是，除非我们“让公司投入大量资金，确保有能力做到这一点的人员遵守法律和更广泛的道德规范，否则我们真的不会取得任何进展。”</p>
<p class="translated">Pasquale 表示，他是政府监管和监督的支持者，并设想成立一个类似于联邦搜索委员会的机构来监管搜索引擎，并分析它们如何对个人和公司进行排名和评级。</p>
<p class="translated">然而，Friedler 看到了这样一种情况，外部组织会开发度量标准来度量他们认为的问题。然后，该组织可以公布这些度量标准及其方法。</p>
<p class="translated">弗里德勒告诉我:“就像许多这类社会福利一样，我们希望看到他们做什么，然后让他们承担责任，这取决于社会的其他成员。”。“我也愿意相信，这些科技公司中有许多都想做正确的事情。但公平地说，确定什么是正确的事情是非常棘手的。衡量它甚至更加棘手。”</p>
<p class="translated">算法不会消失，我想我们都同意，它们只会变得更加普遍和强大。但是，除非学者、技术专家和其他利益相关者确定一个具体的程序，让算法及其背后的科技公司承担责任，否则我们都处于危险之中。</p>
			</div>

			</div>    
</body>
</html>