# 你会让一个算法选择下一任美国总统吗？

> 原文：<https://web.archive.org/web/https://techcrunch.com/2016/11/06/would-you-let-an-algorithm-choose-the-next-u-s-president/>

维亚切斯拉夫·波隆斯基撰稿人

[Vyacheslav](https://web.archive.org/web/20230226013227/http://vyacheslavpolonski.com/#networked-innovation)

是哈佛大学的博士生

[Oxford Internet Institute](https://web.archive.org/web/20230226013227/https://www.oii.ox.ac.uk/)

。他的研究使用社会心理学和机器学习来理解人的网络和思想的网络。

想象一下 2020 年典型的一天:你的个人人工智能助理在准备你最喜欢的早餐之前，用一句友好的问候叫醒你。在你晨练的时候，它会播放完全符合你音乐品味的新歌。对于你的无人驾驶通勤，它已经根据你通勤的持续时间和你过去读过的内容预先选择了几篇文章。

你看了新闻，意识到总统选举即将到来。基于一个预测模型，该模型考虑了你之前表达的观点和你所在州其他选民的数据，你的人工智能助理建议你投票给民主党候选人。你的手机上会弹出一条消息，询问你是否希望你的人工智能助理处理文书工作，并代表你投票。你点击“同意”，继续你的生活。

## 人工智能只和数据一样好

我们正在进入一个充满人工智能的世界。虽然[个人人工智能助理](https://web.archive.org/web/20230226013227/https://en.wikipedia.org/wiki/Intelligent_personal_assistant)已经成为现实，但对许多人来说，我们将会委派如此重要的公民职责似乎是不可思议的——即使人工智能助理可能知道在任何给定的时刻什么对我们最好。如果输入足够的数据，人工智能助手可以给出比我们从甚至是最亲密的朋友那里得到的任何东西都更加准确和个性化的建议。

然而，机器的智能潜力经常引起人们的恐惧。研究显示，34%的人害怕人工智能，而 24%的人认为人工智能对社会有害。GWI 发现 63%的人担心他们的个人数据会被公司利用。我们自己最近在牛津互联网研究所的研究表明，人们不愿将他们的个人生活交给人工智能助理，特别是当助理在没有提供透明推理的情况下做出决定，从一组备选方案中选择一个解决方案时。

没有必要把数学和魔法混为一谈。人工智能不是由生活在你手机里的小的神秘生物操作的，它们有一种代理感和他们自己的议程。但我们容易忘记的是，那些对我们的兴趣、位置、行为、财务和健康做出自动推断的看似不可见的数学模型是由其他人使用我们预先存在的个人数据设计的。

## 人类创造者的角色

当前关于算法文化的辩论大多围绕着人类在算法设计中扮演的角色——也就是说，创作者的潜意识信念和偏见是否被编码到为我们做出决策的算法中。新闻业充满了这样的担忧，即开发者愿意以某种方式编写他们的算法，允许对某些人群进行微妙的歧视，或者更糟的是，技术平台充当了通过它们的信息的看门人。正如[约翰·曼尼斯](https://web.archive.org/web/20230226013227/https://techcrunch.com/2016/09/28/facebook-amazon-google-ibm-and-microsoft-come-together-to-create-historic-partnership-on-ai/)所写的，“一个有偏见的世界会导致有偏见的数据集，反过来，也会有偏见的人工智能框架。”

> 在社会进步方面，对 AI 的快速成长有一种隐性的厌恶。

推进这一论点的政策制定者和专家倾向于曲解算法偏差明显流行的证据。他们责怪算法背后的人，而不是算法本身。[挑别人的毛病](https://web.archive.org/web/20230226013227/http://data.whicdn.com/images/129206401/original.jpg)当然是自然的反应，尤其是如果你不了解手头技术的内部运作。

但算法偏差很少源自其人类创造者。在大多数情况下，它来自用于训练这种算法的数据。这确实是本文开头所描述的未来场景的真正危险。

## 算法决定论

让我们暂停片刻，回忆一下[机器学习](https://web.archive.org/web/20230226013227/https://en.wikipedia.org/wiki/Machine_learning)实际上是如何工作的。通过应用统计学习技术，编码人员开发出可以自动识别数据模式的计算模型。为了实现这一点，模型需要在大型数据集上进行[训练](https://web.archive.org/web/20230226013227/https://en.wikipedia.org/wiki/Supervised_learning)，以挖掘数据中的边界和关系。用于训练模型的数据越多，预测准确度就越高。

在个性化数字应用的背景下，这些统计学习技术被用于为其用户创建算法身份，这包括几个维度，例如使用模式、品味、偏好、个性特征和他们的社交图的结构。然而，这种数字身份并不直接基于用户的人格或自我意识。相反，这种推断是基于可测量数据点的集合以及机器对其的解释。换句话说，无论多么复杂，具体化的用户身份都会被人工智能眼中不完美的数字表示所取代。

但人工智能只能在计算中使用历史追踪数据，然后用于预测用户的需求并对未来做出预测。这就是为什么在用过去(男性)总统的图像训练之后，根据过去美国总统的图像训练的[神经网络](https://web.archive.org/web/20230226013227/http://www.ewdn.com/2016/08/08/neural-networks-see-donald-trump-as-us-president/)预测唐纳德·特朗普将赢得即将到来的美国大选。因为数据集中没有女性总统，人工智能无法推断出性别不是该模型的相关特征。实际上，如果这个特定的人工智能要选举下一任总统，它会投票给特朗普。

这些推论产生了越来越确定的推荐系统，它倾向于强化现有的信念和实践，类似于我们社交媒体反馈中的[回音室](https://web.archive.org/web/20230226013227/https://www.weforum.org/agenda/2016/08/the-biggest-threat-to-democracy-your-social-media-feed)。在这一点上，[亚诺·科波宁](https://web.archive.org/web/20230226013227/https://techcrunch.com/2015/06/25/the-future-of-algorithmic-personalization/)断言“个性化丑化了我们，并在我们的真实兴趣和他们的数字反映之间制造了惊人的差距。”同时， [*《网飞效应*](https://web.archive.org/web/20230226013227/https://books.google.de/books?id=MhBNDAAAQBAJ&printsec=frontcover) 的作者解释说，个人推荐系统倾向于“将用户导向这些内容，从而将用户隔离在人口统计学分类内容的规定类别中。”随着时间的推移，这些方面变得越来越普遍，为什么算法决定论如此有害就变得显而易见了。

## 流动的身份和变化

我们的身份是动态的、复杂的，包含许多矛盾。基于我们的社会背景，我们可能会有不同的行为，并需要我们的人工智能代理提供不同形式的帮助——在学校或工作场所，在酒吧或教堂。

除了我们默认的自我展示，还有很多原因让我们想要扮演不同的身份，以区别对待我们个人网络中的各个子群体。我们是想让我们的整个社交网络都可以接触到我们自己，还是想在远离朋友和家人窥探的地方找到新的未开发的社交空间？如果我们想尝试不同的社会角色和身份，会发生什么？正如创始人 T2 克里斯·普尔所说，“重要的不是你与谁分享；而是你分享的人(…)。身份是棱形的；人们看待你的角度有很多。”

> 问责制在这里是一个重要的问题，正是因为它很难大规模地衡量和实施。

对于人工智能来说，区分自我表现的不同层面并将其映射到各种社交环境是一项极具挑战性的任务，人工智能已经被训练为服务于独特的用户身份。有时候甚至我们都不知道自己是谁。但我们的人工智能助手总会给我们一个答案:这就是我们昨天的样子。改变变得越来越困难，我们的使用模式和信念系统面临着被锁定在自我执行循环中的风险，就像算法土拨鼠日一样。

我们在日常生活中越依赖个性化算法，它们就越会塑造我们看到的东西、我们阅读的内容、我们与谁交谈以及我们的生活方式。通过坚持不懈地关注现状，对要读的书、要看的电影和要结识的人的新建议会给我们带来更多以前让我们高兴的东西。

当你的过去明确地决定了你的未来，通过自发性、开放的思想和实验的个人发展变得更加困难。这样，算法决定论的概念呼应了[温斯顿·邱吉尔](https://web.archive.org/web/20230226013227/http://www.parliament.uk/about/living-heritage/building/palace/architecture/palacestructure/churchill/)曾经对建筑说过的话:我们塑造我们的算法；此后，他们塑造了我们。

## 如何阻止(或减缓)未来

今天，人工智能的现实应用已经嵌入到日常生活的几乎每个方面。但是，有两个主要的挑战阻止了开头几段中概述的潜在未来。

在技术进步方面，应用程序之间的数据交换缺乏互操作性标准，这阻止了激进的个性化。为了真正有用，机器学习系统需要更多的个人数据——这些数据目前被储存在竞争对手的技术公司的专有数据库中。拥有数据的人掌握着权力。一些公司，最著名的是苹果公司和 T2 公司，已经开始通过尝试第三方服务整合来民主化这种力量。最近，一些最大的技术公司宣布了一项重大的[合作伙伴关系，以在人工智能研究](https://web.archive.org/web/20230226013227/http://www.partnershiponai.org/)方面进行合作，这将使许多人受益，而不是少数人。展望未来，这对于建立公众对这项技术的信任至关重要。

在社会进步方面，对 AI 的快速成长有一种隐性的厌恶。人们害怕失去对人工智能助手的控制。为了看似微不足道的生产率提高而拿自己的关系和声誉冒险，这不是大多数人愿意做的事情。当然，在早期阶段，人工智能助理可以以其人类创造者可能不会想到的方式行事。有很多失败的人工智能实验[的先例](https://web.archive.org/web/20230226013227/http://qz.com/653084/microsofts-disastrous-tay-experiment-shows-the-hidden-dangers-of-ai/)削弱了对狭隘的人工智能解决方案和对话机器人的信任。当脸书、微软和谷歌都在 2016 年推出他们的机器人平台时，用户对过早推出的人工智能技术的有限用途、应用和定制感到失望。

许多反乌托邦科幻场景描绘了有意识的流氓人工智能接管世界，这也加剧了对人工智能技术后果的挥之不去的担忧。但我们走向的未来既不是天网，也不是奥威尔式的老大哥:它更有可能看起来像《勇敢新世界》中描绘的享乐社会，在那里，技术通过普遍幸福和无处不在的自我放纵来维持地位。

## 新的监管框架

技术不断进步，但也有希望。全球塑造者社区的 [2016 年度调查显示，年轻人将 AI 视为主要的技术趋势。此外，21%的受访者表示他们支持人形机器人的权利，这种支持在东南亚不成比例地高，那里的年轻人似乎对日常生活中的人工智能持更积极的态度。](https://web.archive.org/web/20230226013227/http://shaperssurvey.org/)

在欧洲，欧盟新的[通用数据保护条例](https://web.archive.org/web/20230226013227/http://ec.europa.eu/justice/data-protection/reform/index_en.htm)可以限制极端形式的算法决定论，让用户有机会要求解释特定的基于分析的算法决策。这部法律预计将于 2018 年 5 月在所有欧盟成员国实施。虽然这一法规[限制了剖析](https://web.archive.org/web/20230226013227/https://iapp.org/news/a/top-10-operational-impacts-of-the-gdpr-part-5-profiling/)并强调了[算法设计中人类可解释性的迫切重要性](https://web.archive.org/web/20230226013227/https://arxiv.org/pdf/1606.08813v3.pdf)，但不确定它是否会导致大型科技公司的主流算法实践发生任何重大变化。

每天都有成千上万个关于我们每个人的算法决策——从网飞电影推荐和脸书朋友建议到保险风险评估和信用评分。考虑到所有的事情，公民自己现在负责跟踪和审查对他们做出的算法决定吗，或者这需要编码到他们使用的数字平台的设计中吗？问责制在这里是一个重要的问题，正是因为它很难大规模地衡量和实施。

现在是时候绘制一条明智的道路，创造透明和负责任的人工智能，以改善人类的集体未来。因此，在一头扎进未知之前，我们应该问自己的问题是，我们希望人类和人工智能之间的关系是什么样的？重新思考这些问题将有望让我们设计出非确定性的、透明的和负责任的算法，这些算法认识到我们个性的复杂、进化和多面性。