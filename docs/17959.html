<html>
<head>
<title>Duplex shows Google failing at ethical and creative AI design | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Duplex显示谷歌在道德和创造性人工智能设计方面的失败</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/05/10/duplex-shows-google-failing-at-ethical-and-creative-ai-design/">https://web.archive.org/web/https://techcrunch.com/2018/05/10/duplex-shows-google-failing-at-ethical-and-creative-ai-design/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">本周在山景城举行的I/O大会上，谷歌首席执行官Sundar  Pichai展示了一个正在开发的语音助手功能，该功能将使人工智能<a href="https://web.archive.org/web/20230403100354/https://techcrunch.com/2018/05/08/the-google-assistant-will-soon-be-able-to-call-restaurants-and-make-a-reservations-for-you/">能够代表其人类主人</a>打电话，赢得了一群热情的家庭草坪开发者的支持。</p>
<p class="translated">谷歌助手的所谓“双工”功能显示，它给一家发廊打电话预订一位女士的发型，给一家餐馆打电话试图预订一张桌子——却被告知不接受少于5人的预订。</p>
<p class="translated">这时，人工智能改变了策略，询问等待时间，让它的所有者和控制者谷歌确信，在选定的时间不会等待很久。任务完成。</p>
<p class="translated">该语音系统部署了听起来像人的声音提示，如“嗯”和“啊”——以使“<a href="https://web.archive.org/web/20230403100354/https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">对话体验更舒适</a>”，因为谷歌在一篇博客中表达了它对该技术的意图。</p>
<p class="translated">谷歌在演示中为人工智能使用的声音不是合成的机器人声音，而是明显的人类声音，无论是女性还是男性。</p>
<p class="embed breakout embed-oembed embed--video embed--youtube"/>
<p class="translated">事实上，人工智能哑剧显然足够逼真，足以让电话另一端的一些真正的人类相信他们正在与人对话。</p>
<p class="translated">在某一点上，机器人的“嗯嗯”回应甚至引来了技术观众的赞赏笑声，他们显然也感受到了这个“玩笑”。</p>
<p class="translated">但是，尽管家庭观众热情欢呼谷歌似乎已经做出了其原型机器人呼叫者——皮查伊继续勾画人工智能节省人们和企业时间的宏伟愿景——这一插曲令人担忧地暗示了一家将道德视为事后考虑的公司。</p>
<p class="translated">它不允许自己的工程创造力受到影响。</p>
<p class="translated">这一考虑似乎只有在现实世界推出的尖端才能在人工智能开发过程中看到——皮查伊说这将很快到来。</p>
<h2 class="translated">故意欺骗</h2>
<p class="translated">“谷歌的实验看起来确实是为了欺骗而设计的，”牛津互联网研究所数字伦理实验室研究员托马斯·金博士在讨论双工演示时表示同意。“因为他们的主要假设是‘你能把这个人和真人区分开来吗？’。在这种情况下，不清楚为什么他们的假设是关于欺骗而不是用户体验…你不一定需要通过听起来自然来欺骗某人以给他们更好的用户体验。如果他们测试的是假设“这项技术比以前的版本更好还是和打电话的人一样好”，他们就不会在实验中欺骗人们。</p>
<p class="translated">“至于技术本身是否具有欺骗性，我真的不能说他们的意图是什么——但是……即使他们不打算欺骗你，你也可以说他们疏忽了，没有确保它不会欺骗……所以我不能说它肯定具有欺骗性，但应该有某种机制让人们知道他们在对什么说话。”</p>
<p class="translated">“我在一所大学，如果你要做一些涉及欺骗的事情，你必须真正证明这样做有科学价值，”他补充说，他同意，作为一项普遍原则，人类应该始终能够知道他们正在互动的人工智能不是人。</p>
<p class="translated">正如他所说，因为你与谁或什么互动“决定了我们的互动方式”。“如果你开始模糊界限……那么这可能会在各种互动中产生不信任——我们会变得更加怀疑，并且用毫无意义的代理人不必要地取代人。”</p>
<p class="translated">然而，没有这样的伦理对话困扰着I/O阶段。</p>
<p class="translated">然而，皮查伊表示，谷歌已经在双工技术上研究了“许多年”，甚至声称人工智能可以“理解对话的细微差别”——尽管显然仍然是在非常狭窄的场景中，例如预约或预订桌子，或者询问企业在特定日期的营业时间。</p>
<p class="translated">“它汇集了我们多年来在自然语言理解、深度学习、文本到语音方面的所有投资，”他说。</p>
<p class="translated">这个列表中令人瞠目结舌的缺失，以及这个棘手的双重实验的设计中似乎也缺乏的是，谷歌对围绕人工智能技术的伦理问题有着深刻而微妙的理解，这些技术强大到足以冒充人类，从而在这个过程中扮演许多真人。</p>
<p class="translated">双工演示是预先录制的，而不是现场电话通话，但皮查伊将这些通话描述为“真实的”——这表明谷歌代表实际上没有提前给企业打电话，警告他们其机器人可能会打电话进来。</p>
<p class="translated">“我们有很多这样的例子，电话并不像预期的那样进行，但我们的助理理解上下文，细微差别…并优雅地处理互动，”他在播出餐厅无法预订的例子后补充道。</p>
<p class="translated">因此，谷歌似乎已经将Duplex训练成具有很强的欺骗性——即能够绕过脱轨的对话预期，同时仍将自己伪装成人类——皮查伊称赞这一特征“优雅”。</p>
<p class="translated">即使人工智能在野外的表现比谷歌的演示更不稳定，但这显然是首席执行官对这项技术的目标。</p>
<p class="translated">虽然trickster人工智能可能会让人想起标志性的图灵测试——聊天机器人开发人员竞相开发能够说服人类法官的对话软件——但这不应该。</p>
<p class="translated">因为双工技术的应用不在高姿态和众所周知的竞争的范围内。也没有一套事先让每个人都知道并同意的规则(至少就我们所知——如果有什么规则谷歌没有公布的话)。相反，它似乎将人工智能释放到了那些刚刚开始日常工作的不知情的业务人员身上。你能看到道德上的脱节吗？</p>
<p class="translated">“图灵测试已经成为测试你的人工智能软件是否优秀的风向标，基于你是否能把它与人类区分开来，”这是金对谷歌为什么可能选择类似的技巧作为Duplex的实验展示的建议。</p>
<p class="translated">“说看看我们的软件有多棒很容易，人们无法将它与真人区分开来——也许这比你说90%的用户更喜欢这个软件而不是以前的软件更有卖点，”他假设道。“脸书做了A/B测试，但这可能没那么令人兴奋——它不会让任何人惊讶地说，消费者更喜欢这种稍微深一点的蓝色，而不是浅一点的蓝色。”</p>
<p class="translated">如果Duplex在图灵测试条件下部署，King还指出，它不太可能接纳这么多人——因为，嗯，那些稍微不合时宜的um和ahs很快就会被发现，就像恐怖谷一样。</p>
<p class="translated">因此，谷歌针对Duplex的公关风格的“人工智能测试”也受到了操纵，以进一步增强围绕人工智能的单向推广营销信息。所以，换句话说，向另一层伪装问好。</p>
<p class="translated">谷歌如何以一种合乎道德的方式引入Duplex？金认为它需要预先声明它是一个机器人，并且/或者使用适当的合成声音，这样任何拿起电话的人都会立刻清楚打电话的人不是人类。</p>
<p class="translated">“如果你使用机器人的声音，你合成的所有声音只代表说BBC英语’的一小部分人的风险也会降低，因此，也许在某种意义上，使用机器人的声音甚至也会更少偏见，”他补充道。</p>
<p class="translated">当然，正如金解释的那样，不预先承认复式公寓是人为的，会带来各种其他连锁风险。</p>
<p class="translated">“如果不能明显看出这是一个机器人的声音，人们就有可能会认为这些电话大部分都不是真的。现在，实验表明，许多人确实与人工智能软件互动，就像他们与另一个人互动一样，但同时也有证据表明，一些人的行为恰恰相反——他们变得更加粗鲁。有时甚至辱骂对话软件。因此，如果你不断地与这些机器人互动，你可能不会像平时那样礼貌，这可能会对你收到一个你不知道是真是假的真正来电者产生潜在影响。或者，即使你知道它们是真实的，也许你与人交往的方式已经发生了一些变化。”</p>
<p class="translated">可以肯定地说，随着自主系统变得越来越强大，能够执行我们通常期望人类执行的任务，围绕这些系统的伦理考虑与潜在的应用一样呈指数级增长。我们才刚刚开始。</p>
<p class="translated">但是，如果世界上最大和最强大的人工智能开发者认为把伦理放在次要位置完全没问题，那么风险就会不断上升，事情可能会变得非常糟糕。</p>
<p class="translated">例如，我们已经看到，微目标广告平台是如何被潜在的选举骗子大规模劫持的。但是，人工智能和自动化技术面临的最大风险是，人类成为二等公民，而不是被声称可以帮助我们的工具。</p>
<p class="translated">皮查伊说，Duplex的第一个——用他的话说，仍然是试验性的——用途将是补充谷歌的搜索服务，在公共假期等时间不方便变化的时段，填充有关企业营业时间的信息。</p>
<p class="translated">虽然对于一家以“组织世界信息并使其普遍可用和有用”为总使命的公司来说，有什么可以阻止谷歌部署大量手机机器人打电话向人类(及其相关企业和机构)询问各种专业知识，然后公司可以自由提取并注入其众多的互联服务中——通过我们额外参与的注意力和它旁边的广告将免费的人类增强英特尔货币化？</p>
<p class="translated">在写这篇文章的过程中，我们几次联系谷歌新闻热线，要求与相关公司发言人讨论Duplex的道德规范。但具有讽刺意味的是——或许也很恰当——我们手写的电子邮件只收到了自动回复。</p>
<p class="translated">皮查伊强调，这项技术仍在开发中，并表示谷歌希望“努力做到这一点，获得用户体验以及企业和用户的期望”。</p>
<p class="translated">但这仍然是伦理作为一个附加的想法——而不是它应该在的地方:锁定在人工智能系统设计的基石的位置。</p>
<p class="translated">与此同时，平台推动的人工智能问题，如算法化的假新闻，已经滚雪球般变成巨大而丑陋的全球丑闻，具有非常深远的社会影响——无论是选举干预还是种族暴力。</p>
<p class="translated">你真的很想知道，怎样才能动摇一些科技行业巨头的“先打破它，再修复它”的风气…</p>

<p class="translated">如果你愿意去读的话，关于谷歌在这里用双工人工智能做什么的伦理指导实际上是非常清楚的——甚至<a href="https://web.archive.org/web/20230403100354/https://techcrunch.com/2016/10/12/white-house-reports-on-ai-no-skynet/">甚至政客</a>都同意<a href="https://web.archive.org/web/20230403100354/https://techcrunch.com/2018/04/16/uk-report-urges-action-to-combat-ai-bias/">的基本原则，比如人工智能需要在“可理解性和公平性原则”上运作，借用近年来发表的几份政治报告中的一份的措辞。</a></p>
<p class="translated">总之，欺骗是不酷的。人类没有。绝对不在应该帮助我们的人工智能里。</p>
<h2 class="translated">作为人工智能标准的透明度</h2>
<p class="translated">2016年底，<a href="https://web.archive.org/web/20230403100354/https://techcrunch.com/2016/12/13/ieee-puts-out-a-first-draft-guide-for-how-tech-can-achieve-ethical-ai-design/"> IEEE </a>技术专业协会发布了第一份框架草案，以指导道德设计的人工智能系统——其中包括一些一般原则，如需要确保人工智能尊重人权，透明地运作，以及自动决策是负责任的。 <span> </span></p>
<p class="translated">同年，英国的BSI标准机构制定了一项特殊的标准——BS 8611道德设计和应用机器人——其中明确地将身份欺骗(有意或无意)列为一种社会风险，并警告说这种方法最终会削弱人们对该技术的信任。</p>
<p class="translated">BSI的标准建议:“避免由于机器人的行为和/或外观造成的欺骗，并确保机器人本性的透明性”。</p>
<p class="translated">它还警告不要拟人化，因为有误解的相关风险——所以Duplex的ums和ahs不只是因为它们是假的，而是因为它们具有误导性和欺骗性，因此也带来了破坏人们对你的服务的信任的连锁风险，而且还会更广泛地影响其他人。</p>
<p class="translated">“避免不必要的拟人化”，这是该标准的一般指导原则，进一步指导该技术“仅用于明确定义的、有限的和社会接受的目的”。(欺骗工人与机器人进行远程对话可能不是他们所想的。)</p>
<p class="translated">该标准还敦促“澄清是否有意模拟人类，或有意或预期的行为”。所以，再一次，不要试图把你的机器人伪装成人类；你需要弄清楚这是一个机器人。</p>
<p class="translated">对于Duplex来说，皮查伊表示，在人工智能开发过程的后期阶段，谷歌现在打算考虑的透明度将非常容易实现:它可以让助手预先说:“你好，我是代表谷歌打电话的机器人——你愿意和我说话吗？”</p>
<p class="translated">相反，谷歌选择优先考虑一个演示“哇”的因素——展示Duplex在繁忙而信任的人类眼睛上拉羊毛——并通过这样做表明自己对道德人工智能设计的话题一无所知。</p>
<p class="translated">对谷歌来说不是什么好消息。对于我们这些受制于科技巨头在社交平台上轻触控制开关的算法突发奇想的人来说，这也不是一个好的前景。</p>
<p class="translated">“随着人工智能系统的发展和更多研究的开展，强调与它们的使用相关的道德风险并将其作为设计的一部分来考虑是很重要的，”BSI制造部门负责人丹·帕尔默告诉我们。“BS 8611是由科学家、学者、伦理学家、哲学家和用户共同开发的。它解释说，任何自主系统或机器人都应该是负责任的、真实的和不带偏见的。</p>
<p class="translated">“该标准提出了许多与谷歌双重身份相关的潜在道德风险；其中之一是，由于有偏见的数据反馈，人工智能机器有可能成为性别歧视者或种族主义者。当Twitter用户影响微软的人工智能聊天机器人Tay发出攻击性信息时，这一点浮出水面。</p>
<p class="translated">另一个有争议的话题是，与机器人建立情感纽带是否可取，尤其是当语音助手与老人或孩子互动时。其他应该考虑的新危险指南包括:机器人欺骗、机器人成瘾和学习系统超出其职权范围的可能性。</p>
<p class="translated">“最终，它必须始终透明，谁对任何语音助手或机器人的行为负责，即使它是自主行为。”</p>
<p class="translated">然而，尽管已经产生了所有深思熟虑的道德指导和研究，并且供读者阅读，但我们在这里再次看到了同样疲惫的技术行业剧本，在闪亮的泡沫中称赞工程能力，剥离了人类背景和社会考虑，并在不加批判的观众面前晃来晃去，看他们会欢呼得多大声。</p>
<p class="translated">留下重要的问题——关于谷歌人工智能实验的道德，更广泛地说，关于它如此热衷于向我们推销的人工智能辅助的主流愿景——悬而未决。</p>
<p class="translated">像它告诉我们我们都想使用的人工智能应用程序可能有多少真正的效用这样的问题，即使它准备将这些应用程序推给我们，因为它可以——这是其强大的平台能力和影响力的结果。</p>
<p class="translated">一个核心的“诡异的山谷”似的悖论可以解释谷歌选择欺骗其双工演示:人类不一定喜欢和机器说话。事实上，他们经常喜欢和其他人说话。让一个脉冲携带者记录你的存在更有意义。因此，如果一个人工智能显示自己是一个机器人，拿起电话的人很可能会直接把它放回去。</p>
<p class="translated">“回到欺骗，如果它取代了无意义的互动，这是好的，但如果它打算取代有意义的互动，就不好了，”金告诉我们。“所以，如果很明显它是人工合成的，你不一定能在人们真的想让人类做那份工作的情况下使用它。我认为这是正确的做法。</p>
<p class="translated">“重要的不仅仅是你的理发师看起来在听你说话，而是他们真的在听你说话，他们反映了你的一些情绪。而用合成的东西来代替那种工作——我觉得没多大意义。</p>
<p class="translated">“但同时，如果你透露它是合成的，它不太可能取代那种工作。”</p>
<p class="translated">因此，谷歌的双重花招实际上可能是试图掩盖这样一个事实，即人工智能将无法取代技术专家认为它们将取代的那么多人类任务。除非许多目前有意义的互动变得毫无意义。这将是一个巨大的人类成本，社会将不得不——至少——进行长期而艰苦的辩论。</p>
<p class="translated">希望通过假装在这里看不到任何道德的东西来避免这样的争论发生，这不是谷歌的设计意图。</p>
<p class="translated">金还指出，双工系统(至少目前)计算成本很高。“这意味着谷歌不能也不应该把它作为任何人都可以在家用电脑上运行的软件发布。</p>
<p class="translated">“这意味着他们还可以控制如何使用它，以及在什么环境中使用它，并且他们还可以保证只在内置了某些安全措施的情况下使用它。因此，我认为这些实验可能不是最好的迹象，但真正的考验将是他们如何发布它——以及他们是否会在软件中建立人们需要的保护措施，”他补充道。</p>
<p class="translated">除了在双工演示中缺乏可见的安全措施之外，我还认为展示中缺乏想象力。</p>
<p class="translated">如果谷歌足够大胆地透露其机器人对话者，它可能会更多地考虑如何设计这种体验，使其既明显不是人类，又有趣，甚至滑稽。想想看，有多少生命可以注入到动画角色中，例如，这些角色很明显不是人类，但却非常受欢迎，因为人们觉得他们很有趣，觉得他们以自己的方式活了过来。</p>
<p class="translated">这真的让你想知道，在某些基础层面上，谷歌是否对人工智能技术的能力以及它自己为这些涌现的合成体验注入新生命的创造能力缺乏信任。</p>
			</div>

			</div>    
</body>
</html>