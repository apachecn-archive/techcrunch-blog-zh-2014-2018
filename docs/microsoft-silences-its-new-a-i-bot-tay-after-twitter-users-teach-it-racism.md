# 在推特用户教微软种族主义后，微软沉默了它的新人工智能机器人 Tay 更新] 

> 原文：<https://web.archive.org/web/https://techcrunch.com/2016/03/24/microsoft-silences-its-new-a-i-bot-tay-after-twitter-users-teach-it-racism/>

微软新推出的名为 Tay 的人工智能机器人(T0 ),在 GroupMe 和 Kik 上回复推文和聊天，由于担心无法识别何时发表了攻击性或种族主义言论，已经被关闭。当然，机器人并没有把*编码为*种族主义者，但是它从和它互动的人身上“学习”。自然，考虑到这是互联网，在线用户教泰的第一件事就是如何成为种族主义者，以及如何反唇相讥不明智或煽动性的政治观点。[ **更新**:微软现在表示，鉴于这个问题，它正在对 Tay 进行“调整”。]

万一你错过了, [Tay 是一个人工智能项目](https://web.archive.org/web/20230406183903/https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/)由微软技术和研究团队以及 Bing 团队建立，致力于对会话理解进行研究。也就是说，这是一个你可以在网上聊天的机器人。该公司将机器人描述为“微软的互联网之家，一个没有寒意的互联网！”，如果你能相信。

Tay 能够完成许多任务，比如给用户讲笑话，或者对你发给她的照片发表评论。但她也旨在个性化与用户的互动，同时回答问题，甚至将用户的陈述反映给他们。

随着推特用户很快理解，泰经常会用她自己的评论重复种族主义推特。除了内容本身之外，令人不安的是，泰的回答是由包括即兴喜剧演员在内的工作人员做出的。这意味着，即使她在推特上发出冒犯性的种族诽谤，她似乎也是带着抛弃和冷漠这样做的。

此后，微软删除了一些最具破坏性的推文，但一个名为[Socialhax.com](https://web.archive.org/web/20230406183903/http://www.socialhax.com/2016/03/24/microsoft-creates-ai-bot-internet-immediately-turns-racist/)的网站在删除前收集了其中一些推文的截图。许多推特都看到泰勒提及希特勒，否认大屠杀，支持特朗普的移民计划(建造一堵墙)，甚至站在# GamerGate 丑闻的施虐者一边。

[gallery id = " 1296691，1296690，1296695，1296694，1296693，1296692"]

这并不是微软在推出机器人通过社交网络与千禧一代用户聊天时所希望的体验。

一些人指出，网络用户和 Tay 之间对话的转移支持了被称为“[戈德温定律”的互联网格言。](https://web.archive.org/web/20230406183903/https://en.wikipedia.org/wiki/Godwin%27s_law)“这表明随着网上讨论的时间越来越长，涉及纳粹或希特勒的比较的可能性越来越大。

但它真正表明的是，虽然技术既不是好的也不是坏的，但工程师有责任确保它的设计不会反映出人性最糟糕的一面。对于在线服务，这意味着在你邀请大众加入之前，反滥用措施和过滤应该一直到位。对于像 Tay 这样的东西，你不能跳过教机器人“不”说什么的部分。

微软显然意识到了泰的种族主义问题，并在周三晚些时候，经过 16 个小时的聊天后，关闭了该机器人。Tay 通过推特宣布，她晚上要关机，但她还没有重新开机。

~~微软被要求置评，但迄今拒绝回应。该公司也没有就该项目是否会无限期离线发表任何声明。~~

**更新**:微软发言人现在证实已经暂时将 Tay 下线，并正在进行调整:

> “人工智能聊天机器人 Tay 是一个机器学习项目，旨在与人类互动。这既是一项技术实验，也是一项社会和文化实验。不幸的是，在上线后的 24 小时内，我们发现一些用户滥用 Tay 的评论技巧，让 Tay 以不恰当的方式做出回应。因此，我们已经让 Tay 离线，并正在进行调整。”