<html>
<head>
<title>Unbiased algorithms can still be problematic • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无偏的算法仍然存在问题 TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/09/30/unbiased-algorithms-can-still-be-problematic/">https://web.archive.org/web/https://techcrunch.com/2018/09/30/unbiased-algorithms-can-still-be-problematic/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">创造无偏见的、精确的算法不是不可能的——只是很耗时。</p>
<p class="translated">“这实际上在数学上是可能的，”面部识别初创公司 Kairos 首席执行官布莱恩·布莱肯在 TechCrunch Disrupt SF 的一个小组讨论会上告诉我。</p>
<p class="translated">算法是计算机为了解决问题并对特定的行动过程做出决策而遵循的一套规则。无论是我们接收到的信息类型，人们看到的关于我们的信息，我们受雇做的工作，我们获得批准的信用卡，以及未来看到或看不到我们的无人驾驶汽车，算法正日益成为我们生活的一大部分。但算法有一个固有的问题，它从最基础的层面开始，并在整个适应过程中持续存在:这些基于机器的决策者中存在人类偏见。</p>
<p class="translated">创建无偏的算法是一个拥有足够精确数据的问题。这不仅仅是在模型中有足够多的“苍白男性”，而是有足够多的来自不同种族背景、性别、能力、身高、体重等等的人的图像。</p>
<p/><div id="attachment_1722887" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-1722887" loading="lazy" class="breakout wp-image-1722887 size-full" src="../Images/1957d366b5d96af8defe90c5efbc9e26.png" alt="" srcset="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg 2048w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=150,100 150w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=300,200 300w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=768,512 768w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=680,453 680w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=1536,1024 1536w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=1200,800 1200w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg?resize=50,33 50w" sizes="(max-width: 2048px) 100vw, 2048px" data-original-src="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820546944_be6956d78e_k.jpg"/><p id="caption-attachment-1722887" class="wp-caption-text translated">凯罗斯首席执行官布莱恩·布莱肯</p></div>
<p class="translated">"在我们的世界里，面部识别完全是人类的偏见，对吗？"布雷肯说。”<span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted">所以</span> <span class="transcript-snippet__content__body__word ng-star-inserted">你</span> <span class="transcript-snippet__content__body__word ng-star-inserted">想</span> <span class="transcript-snippet__content__body__word ng-star-inserted">关于</span> <span class="transcript-snippet__content__body__word ng-star-inserted">艾</span><span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted">它的</span><span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted">它的</span> <span class="transcript-snippet__content__body__word ng-star-inserted">就像</span><span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted">子</span><span class="transcript-snippet__content__body__word ng-star-inserted"/>你<span class="transcript-snippet__content__body__word ng-star-inserted">教</span> <span class="transcript-snippet__content__body__word ng-star-inserted">它的<span class="transcript-snippet__content__body__word ng-star-inserted">W </span> <span class="transcript-snippet__content__body__word ng-star-inserted">帽子</span> <span class="transcript-snippet__content__body__word ng-star-inserted">我们</span> <span class="transcript-snippet__content__body__word ng-star-inserted">叫</span> <span class="transcript-snippet__content__body__word ng-star-inserted">右</span> <span class="transcript-snippet__content__body__word ng-star-inserted">下</span> <span class="transcript-snippet__content__body__word ng-star-inserted">中</span><span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted"/><span class="transcript-snippet__content__body__word ng-star-inserted">右</span> <span class="transcript-snippet__content__body__word ng-star-inserted">下</span>公平的说法是‘苍白的男性’。非常非常好。非常非常擅长识别符合这种分类的人。"</span></p>
<p class="translated">但是，你离苍白的男性越远，加上女性、不同种族的人等等，“人工智能系统就越难做对，或者至少有信心做对，”布莱肯说。</p>
<p class="translated">尽管如此，即使是百分之百准确的模型也有缺点。在专业方面，一个完全准确的算法的良好面部识别用例将是在会议中心，在那里你可以使用该系统快速识别和验证人们是他们所说的人。这是 Kairos 的一种用例类型，它与企业业务合作，处理身份验证和地址。</p>
<p class="translated">“因此，如果我们错了，在最坏的情况下，你可能不得不再次向你的银行账户转账，”他说。“如果我们错了，也许你看不到在游轮上拍摄的照片。但是，当政府在面部识别方面出错，某人的生命或自由受到威胁时，他们可能会把你放在一个你不应该在的队列中。他们可能会说这个人是罪犯，但他们不是。”</p>
<p class="translated">但是在执法的情况下，无论这些算法多么准确和公正，面部识别软件在执法中没有业务，Brackeen 说。这是因为潜在的非法，过度监视公民。</p>
<p class="translated">鉴于政府已经有了我们的护照照片和身份照片，“他们可以在主要街道上安装摄像头，了解每一个开车经过的人，”布莱肯说。</p>
<p class="translated">这是很有可能的。在上个月，Brackeen 说，Kairos 拒绝了国土安全部的政府请求，为移动汽车后面的人寻求面部识别软件。</p>
<p class="translated">“对我们来说，这是完全不可接受的，”布莱肯说。</p>
<p class="translated">人权数据分析小组首席统计师克里斯蒂安·卢姆(Kristian Lum)在小组讨论会上说，百分之百完美的数学预测的另一个问题是，它归结于模型预测的内容。</p>
<p/><div id="attachment_1708009" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-1708009" loading="lazy" class="breakout wp-image-1708009" src="../Images/26e3549e5d20ee0b5de7ab660a8068a3.png" alt="" srcset="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg 2048w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=150,100 150w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=300,200 300w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=768,512 768w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=680,453 680w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=1536,1024 1536w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=1200,800 1200w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg?resize=50,33 50w" sizes="(max-width: 1024px) 100vw, 1024px" data-original-src="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/43820547344_8b67df85f7_k.jpg"/><p id="caption-attachment-1708009" class="wp-caption-text translated">人权数据分析小组首席统计师克里斯蒂安·卢姆</p></div>
<p class="translated">“通常，在很多这类案件中，你试图预测的事情是类似于再次逮捕，”Lum 说。“因此，即使我们完全能够预测这一点，我们仍然面临的问题是，人类或系统或机构的偏见正在产生有偏见的逮捕。所以，你仍然必须把你 100%的准确性放在上下文中，数据真的测量了你认为它测量的东西吗？数据本身是通过公平的过程产生的吗？”</p>
<p class="translated">HRDAG 大学研究部主任 Patrick Ball 同意 Lum 的观点，他认为，将它从个人层面的偏见移开，而称之为制度或结构层面的偏见，可能更实际。他说，举例来说，如果一个警察部门确信它需要对一个社区比对另一个社区进行监管，那么这个警察是否是一个种族主义者就不那么重要了。</p>
<p/><div id="attachment_1722890" class="wp-caption alignnone"><img aria-describedby="caption-attachment-1722890" loading="lazy" class="breakout wp-image-1722890" src="../Images/0859501ff60b8903339691dfc745d213.png" alt="" srcset="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg 2048w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=150,100 150w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=300,200 300w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=768,512 768w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=680,453 680w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=1536,1024 1536w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=1200,800 1200w, https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg?resize=50,33 50w" sizes="(max-width: 1024px) 100vw, 1024px" data-original-src="https://web.archive.org/web/20221025222316im_/https://techcrunch.com/wp-content/uploads/2018/09/29600719087_7ee07b715b_k.jpg"/><p id="caption-attachment-1722890" class="wp-caption-text translated">HRDAG 研究部主任帕特里克·鲍尔</p></div>
<p class="translated">“相关的是，警察局已经做出了一项制度性决定，对该社区进行过度监管，从而在该社区产生更多的警察互动，从而使具有该邮政编码的人更有可能被归类为危险分子，如果他们被风险评估算法归类的话，”鲍尔说。</p>
<p class="translated">他说，即使警方掌握了每起犯罪的完整信息，为了建立一个公平的机器学习系统，“我们也需要生活在一个监控完善的社会中，这样警方对每起犯罪都有绝对的了解，不会排除任何事情。”“这样就不会有偏见了。让我告诉你，这比一群罪犯逍遥法外还要糟糕。因此，也许我们应该致力于改革警察实践，忘记所有机器学习的干扰，因为它们真的让事情变得更糟，而不是更好。”</p>
<p class="translated">他补充道，“为了公平的预测，你首先需要一个公平的刑事司法系统。我们还有很长的路要走。”</p>
<p class="embed breakout embed--video embed--youtube translated"><iframe title="Dismantling Algorithmic Bias with Patrick Ball, Brian Brackeen and Kristian Lum" src="https://web.archive.org/web/20221025222316if_/https://www.youtube.com/embed/Zx327AwHJYk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
			</div>

			</div>    
</body>
</html>