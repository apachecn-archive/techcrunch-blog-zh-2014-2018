<html>
<head>
<title>YouTube releases its first report about how it handles flagged videos and policy violations • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YouTube发布第一份关于如何处理标记视频和政策违规的报告TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/04/23/youtube-releases-its-first-report-about-how-it-handles-flagged-videos-and-policy-violations/">https://web.archive.org/web/https://techcrunch.com/2018/04/23/youtube-releases-its-first-report-about-how-it-handles-flagged-videos-and-policy-violations/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">YouTube发布了第一份季度<a href="https://web.archive.org/web/20230103212238/https://transparencyreport.google.com/youtube-policy">社区指导方针执行报告</a>，并推出了<a href="https://web.archive.org/web/20230103212238/https://youtube.googleblog.com/2017/12/expanding-our-work-against-abuse-of-our.html">报告仪表板</a>，让用户看到他们标记为审查的视频的状态。涵盖2017年最后一个季度的首份报告跟进了<a href="https://web.archive.org/web/20230103212238/https://youtube.googleblog.com/2017/12/expanding-our-work-against-abuse-of-our.html">在去年12月</a>做出的承诺，即在如何处理虐待和决定哪些视频将被删除方面向用户提供更多透明度。</p>
<p class="translated">“这种定期更新将有助于显示我们在从我们的平台上删除违规内容方面取得的进展，”该公司在其官方博客上的一篇文章中说。“到今年年底，我们计划完善我们的报告系统并添加更多数据，包括关于评论、速度或删除以及策略删除原因的数据。”</p>
<p class="translated">但这份报告不太可能平息一些人的抱怨，他们认为YouTube的规则是随意适用的，目的是为了安抚广告客户，他们对自己的广告在含有暴力极端内容的视频之前播放感到不安。在去年《泰晤士报》的一篇报道之后，这个问题被推到了风口浪尖，但是许多内容创作者说YouTube的更新政策使得在这个平台上赚钱变得非常困难，尽管他们的视频并没有违反它的规则。</p>
<p class="translated">然而，YouTube声称，其反滥用机器学习算法(它依赖该算法来大规模监控和处理潜在的违规行为)正在“在高风险、低量领域(如暴力极端主义)和高量领域(如垃圾邮件)获得回报。”</p>
<p class="translated">其报告称，YouTube在2017年最后一个季度删除了820万个视频，其中大部分是垃圾邮件或包含成人内容。其中，670万首先被其反滥用算法自动标记。</p>
<p class="translated">在一个人报告的视频中，有110万个被YouTube的<a href="https://web.archive.org/web/20230103212238/https://support.google.com/youtube/answer/7554338?ref_topic=7124235">可信标记程序</a>的成员标记，该程序包括个人、政府机构和非政府组织，他们接受了该平台的信任&amp;安全和公共政策团队的培训。</p>
<p class="translated">YouTube的报告positions将删除前收到的视频视为其反虐待措施成功的基准。2017年初，8%因暴力极端主义内容而被删除的视频在达到10次浏览量之前被删除。然而，在YouTube于2017年6月开始使用其机器学习算法后，该公司表示，这一比例增加到了50%以上(在一个脚注中，YouTube澄清说，这些数据不包括在发布前被自动标记并因此没有被观看的视频)。从10月到12月，该平台上所有自动标记的视频中有75.9%在获得任何观看量之前就被删除了。</p>
<p class="translated">在同一时期，人们标记了930万个视频，其中近95%来自YouTube用户，其余来自其信任的Flagger计划和政府机构或非政府组织。人们可以在标记视频时选择原因。大多数被标记为色情内容(30.1%)或垃圾邮件(26.4%)。</p>
<p class="translated">去年，YouTube表示，希望到2018年底，将谷歌“致力于解决违规内容”的人数增加到1万人。现在它说它几乎已经达到了这个目标，并且雇佣了更多的全职反虐待专家，扩大了他们的地区团队。它还声称，机器学习算法的加入使更多的人能够观看视频。</p>
<p class="translated">在其报告中，YouTube提供了更多关于这些算法如何工作的信息。</p>
<p class="translated">“关于检测极端主义内容的自动化系统，我们的团队已经手动审查了超过200万个视频，以提供大量的训练样本，这改善了机器学习标记技术，”它说，并补充说，它已经开始将该技术应用于其他内容违规。</p>

<p class="translated">YouTube的报告可能不会减轻内容创作者的担忧，他们在被他们称为“<a href="https://web.archive.org/web/20230103212238/http://nymag.com/selectall/2017/12/can-youtube-survive-the-adpocalypse.html"> Adpocalpyse </a>”的时期看到自己的收入下降，或者帮助他们找到如何再次成功赚钱的方法。另一方面，这是人们的胜利，包括自由言论活动人士，他们呼吁社交媒体平台在如何处理被标记的内容和违反政策的行为方面更加透明，并可能给脸书和推特带来更多压力。</p>
			</div>

			</div>    
</body>
</html>