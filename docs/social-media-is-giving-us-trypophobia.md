# 社交媒体让我们患上了恐台症

> 原文：<https://web.archive.org/web/https://techcrunch.com/2018/01/27/social-media-is-giving-us-trypophobia/>

科技状态下有些东西烂了。

但是，在所有对假新闻的绝望中，在选举扭曲克里姆林宫虚假信息阴谋的呼声中，在政界呼吁科技巨头找到社会良知的呼声中，一个更棘手的认识正在形成。

假新闻和虚假信息只是一些什么是错的，什么是坏的症状。平台巨头的问题是更根本的。

问题是这些强大的算法引擎是黑盒。而且，在业务方面，每个用户只能看到每个用户看到的内容。

社交媒体的最大谎言是声称它向我们展示了世界。以及他们的后续欺骗:他们的技术产品让我们更亲近。

事实上，社交媒体不是望远镜——正如电话实际上是——而是意见分裂棱镜，它通过用越来越集中的过滤泡沫取代共享的公共领域及其动态重叠的话语来粉碎社会凝聚力。

社交媒体不是结缔组织，而是工程分割，将每一对人类眼球视为一个离散的单元，可以从其同伴中取出并分离出来。

想想看，这是一个恐锥症患者的噩梦。

或者相反的全景监狱——每个用户被关进一个单独的牢房，由平台控制器的有色玻璃塔监视。

毫不奇怪，谎言通过产品传播和膨胀得如此之快，不仅大大加快了信息传播的速度，还故意让人们陷入自己的偏见之中。

首先它迎合，然后它极化，然后它把我们分开。

当我们登录脸书或查看谷歌上的个性化搜索结果时，我们不再是透过黑暗的镜头，而是被单独绑在一个定制的耳机上，连续播放一部定制的电影——在黑暗中，在一个单座影院里，没有任何窗户或门。

你感到幽闭恐惧症了吗？

这是一部算法引擎相信你会喜欢的电影。因为它找出了你最喜欢的演员。它知道你倾向于什么类型。让你夜不能寐的噩梦。你早上想到的第一件事。

它知道你的政治，你的朋友是谁，你去哪里。它不停地观察你，把这种智慧打包成一个为你定制的、量身定做的、不断迭代的、扣人心弦的产品。

它的秘方是你个人喜恶的无限混合，是你无意中从互联网上收集到的。(你的线下习惯也不能幸免于它的收获——向数据经纪人告密也是有好处的。)

没有其他人会看到这部电影。甚至不知道它的存在。没有广告宣布它的放映。为什么要为一部专为你制作的电影竖起广告牌呢？无论如何，个性化的内容几乎可以保证把你绑在座位上。

如果社交媒体平台是香肠工厂，我们至少可以在送货卡车驶出大门时拦截它，探测每个包装内肉色物质的化学成分——并发现它是否真的像他们声称的那样可口。

当然，我们仍然需要这样做上千次，以获得关于每个定制小袋中被输送了什么的有意义的数据。但这是可以做到的。

唉，平台不涉及这样的物理产品，也没有留下这样的物理痕迹让我们去调查。

## 欺骗性行为

理解平台的信息形成过程需要访问它们的算法黑盒。但这些都被锁在公司总部里——在标有“专有”的大招牌后面。没有访客！商业敏感 IP！'

只有工程师和业主可以窥视。甚至他们也不一定总能理解他们的机器正在做出的决定。

但这种不对称有多可持续？如果我们，更广泛的社会——平台依赖谁获得数据、眼球、内容和收入；我们*是*他们的商业模式——看不到我们是如何被他们一个个灌输给我们的东西分割的，我们如何判断技术正在对我们所有人做什么？弄清楚它是如何系统化和重塑社会的？

我们怎么能指望衡量它的影响呢？除非我们在何时何地感受到它的伤害。

如果无法获得有意义的数据，我们如何判断在这里或那里或任何这些迎合偏见的广告平台上花费的时间是否可以被称为“[时间花得值](https://web.archive.org/web/20230306043022/https://techcrunch.com/2018/01/11/facebook-time-well-spent/)”？

举个例子，当一个火车站不得不张贴告示，警告父母不要再看他们的智能手机，而是把目光转向他们的孩子时，科技巨头对我们的吸引力告诉了我们什么？

社会突然刮起了一股新的白痴风吗？还是我们被不公平地夺走了注意力？

当科技公司的首席执行官们承认他们不想让自己家里的孩子靠近他们向其他人推销的产品时，我们应该怎么想？这听起来像是他们认为[这种东西可能是新的尼古丁](https://web.archive.org/web/20230306043022/https://www.youtube.com/watch?v=KDqoTDM7tio)。

外部研究人员一直在尽力绘制和分析在线意见和影响力的流向，试图量化平台巨头的社会影响。

然而，Twitter 却主动贬低这些努力，从其看门人的位置上挑挑拣拣——通过声称图片有缺陷，因为它不完整，来诋毁任何它不喜欢的研究结果。

为什么？因为外部研究人员无法接触到它的所有信息流。为什么？因为他们看不到 Twitter 的算法是如何塑造数据的，也看不到每个 Twitter 用户可能(或可能不会)触动了内容抑制开关，这也可以——Twitter 说——塑造香肠并确定谁消费它。

为什么不呢？因为 Twitter 没有给外人那样的权限。对不起，你没看见标志吗？

当政客们迫使公司提供完整的图片时——基于只有 Twitter 能看到的数据——他们只是得到了更多由 Twitter 公司自身利益塑造的自选碎片。

(这个“解决一个棘手的问题”/“隐藏难看的痣”的游戏可以一直玩下去。然而，从长期来看，这似乎也不是一个非常具有政治可持续性的方案——不管有多少问答游戏可能会突然重新流行起来。)

当脸书公司被证明未能坚持其现有的广告标准时，我们怎么能相信该公司会围绕政治广告建立健全和严格的披露制度呢？

马克·扎克伯格希望我们相信他会做正确的事情。然而，他也是一位强有力的科技公司首席执行官，他故意忽略了恶意造谣在其平台上猖獗的担忧。他们甚至忽视了假新闻可能影响民主的具体警告——这些警告来自一些相当有见识的[政治圈内人士](https://web.archive.org/web/20230306043022/http://thehill.com/news-by-subject/technology/352182-obama-warned-zuckerberg-about-fake-news-on-facebook-report)和[导师](https://web.archive.org/web/20230306043022/https://washingtonmonthly.com/magazine/january-february-march-2018/how-to-fix-facebook-before-it-fixes-us/)。

## 有偏差的黑盒

在假新闻成为脸书商业的生存危机之前，扎克伯格对任何提出的内容担忧的标准防线是偏转——那句臭名昭著的声明“我们不是媒体公司；我们是一家科技公司。

事实证明也许他这么说是对的。因为也许大型科技平台真的需要一种新型的定制监管。这反映了他们的工厂正在大量生产的个性化产品的独特的超目标性质——*恐锥症患者现在看向别处！* — 4BN+眼球量表。

近年来，有人呼吁监管机构能够访问算法黑盒，以揭开作用于我们但我们(产品)无法看到(从而无法监督)的引擎的盖子。

人工智能的使用越来越多，这无疑让这种情况变得更为强烈，如果技术平台盲目地进入商业特权黑箱，偏见的风险就会像技术平台一样迅速蔓延。

我们认为将劣势自动化是正确和公平的吗？至少在抱怨变得足够大声和过分，以至于某个地方有足够影响力的人注意到并大叫犯规之前？

算法问责不应该意味着需要大量的人类痛苦来对技术故障进行逆向工程。我们绝对应该要求适当的程序和有意义的问责制。无论如何都要到达那里。

如果强大的平台每次被要求提供远远超出其自身商业利益的问题的答案时都被认为是拖后腿和塑造真相的——让我再次强调，这些答案只有*他们持有——那么要求打开他们的黑匣子的呼声将成为一种喧嚣，因为他们将获得令人满意的公众支持。*

立法者已经注意到了算法责任这个词。这是他们的口头禅。风险正在被阐明。现存的危害正在被权衡。算法黑盒正在失去其偏斜的公众光泽——平台巨头的巨大超个性化实验已经进行了十多年。

现在没有人会怀疑这些平台影响和塑造了公共话语。但是，可以说，近年来，它们让公共街道变得更粗糙、更愤怒、更容易愤怒、更没有建设性，因为算法奖励了最擅长游戏的巨魔和挑衅者。

因此，只要有足够多的人——足够多的“用户”——加入这些点，并意识到是什么让他们在网上感到如此不安和不安——这些产品就会像其他人[在](https://web.archive.org/web/20230306043022/https://techcrunch.com/2013/06/12/bring-the-blogs-back/)之前一样枯萎。

这也没有工程上的变通方法。即使生殖型人工智能如此擅长虚构内容，以至于它们可以替代人类大量的辛勤劳动，它们仍然永远不会拥有生物眼球，而这是科技巨头赖以生存的广告收入所必需的。(短语“用户生成的内容平台”实际上应该以未提及但非常突出的一点“和用户消费”结尾。)

本周，英国首相特里萨·梅在达沃斯世界经济论坛上发表演讲，抨击[社交媒体平台](https://web.archive.org/web/20230306043022/https://techcrunch.com/2018/01/25/telegram-and-social-media-giants-spanked-in-uk-pms-davos-speech/)未能本着社会良知运营。

在抨击了脸书、推特和谷歌之后——正如她所说，它们助长了[虐童](https://web.archive.org/web/20230306043022/https://techcrunch.com/2017/03/07/facebooks-content-moderation-system-under-fire-for-child-safety-failures/)、[现代奴隶制](https://web.archive.org/web/20230306043022/https://techcrunch.com/2017/08/25/facebook-faces-another-moderation-scandal-over-migrant-torture-videos/)以及传播[恐怖分子](https://web.archive.org/web/20230306043022/https://techcrunch.com/2017/09/19/twitter-claims-tech-wins-in-quashing-terror-tweets/)和[极端主义内容](https://web.archive.org/web/20230306043022/https://techcrunch.com/2017/12/19/twitter-finally-boots-hate-group-that-trump-retweeted-off-its-platform/)——她指出爱德曼[的一项调查](https://web.archive.org/web/20230306043022/https://www.edelman.com/trust-barometer)显示了全球对社交媒体的信任度下降(同时对新闻业的信任度也大幅上升)。

她的潜台词很清楚:就科技巨头而言，世界领导人现在觉得既愿意也有能力磨刀霍霍。

她也不是唯一一个炙手可热的社交媒体演讲者。

“脸书和谷歌已经发展成为越来越强大的垄断企业，它们已经成为创新的障碍，它们引发了各种各样的问题，我们现在才开始意识到这些问题，”美国亿万富翁慈善家乔治·索罗斯(George Soros)表示，他呼吁监管机构采取行动，打破平台对我们的束缚。

虽然政客(和记者——很可能还有索罗斯)习惯了被彻底憎恨，但科技公司肯定不会。这些公司多年来一直沐浴在“创新”这个词的光环之下。“主流反弹”不在他们的字典里。就像“社会责任”直到最近才出现。

你只需看看扎克伯格脸上刻着的忧虑纹，就能明白硅谷的男孩之王们在应对汹涌的公众愤怒时准备得有多么充分。

## 猜谜游戏

大型科技平台的不透明性还有另一个有害的、非人性化的影响——不仅是对他们的数据挖掘用户，也对他们的内容创作者。

像 YouTube 这样的平台，依赖于一个志愿者制造商大军来保持内容在无数屏幕上流动，这些屏幕从其平台上拉走了数十亿的信息流(并将数十亿的广告美元流入谷歌的金库)，但在运营中，它与它的创造者之间拉下了一个不透明的屏幕。

YouTube 有一套内容政策，称其内容上传者必须遵守。但是谷歌并没有始终如一地执行这些政策。媒体丑闻或广告商抵制可能会引发突然爆发的执法行动，让创作者争先恐后地不被冷落。

一位创作者最初接触 TechCrunch 是因为她在一个关于[潮汐舱挑战](https://web.archive.org/web/20230306043022/https://techcrunch.com/2018/01/18/youtube-is-pulling-tide-pod-challenge-videos/)的讽刺视频上获得了安全罢工，她将被 YouTube 的高度自动化系统管理描述为“无处不在的头痛”和毫无人性的猜谜游戏。

Aimee Davison 告诉我们:“我在 YouTube 上的大多数问题都是自动评级、匿名标志(被滥用)和匿名电子邮件支持的匿名、模糊帮助的结果，这些匿名电子邮件支持的纠正能力有限。”“要改善 YouTube 上的合作伙伴关系，需要直接的人际互动和协商，以及清晰明确的一致指导方针。”

“YouTube 需要在不进行过度艺术审查的情况下对其内容进行适当评级——他们需要人性化我们的账户管理，”她补充道。

然而，在管理最受瞩目的内容创作者方面，T2 甚至没有 T4 做得好。又名“YouTube 明星”。

但是，当“明星”YouTube 创始人罗根·保罗——谷歌广告平台的前优先合作伙伴——上传一段他自己在一名自杀受害者尸体旁开玩笑的视频时，真正的责任在哪里？

保罗必须管好自己的良心。但指责也必须超越任何一个在平台上被算法管理(理解为操纵)的个人，这些人生产的内容实际上丰富了谷歌，因为人们受到其奖励系统的引导。

在保罗的案例中，YouTube 的工作人员也对他的视频进行了人工审核和批准。因此，即使 YouTube 声称它有人类眼球来审查内容，这些眼球似乎没有足够的时间和工具来完成这项工作。

考虑到这项任务的艰巨性，这并不奇怪。

谷歌表示，今年将把执行审核和其他强制职责的员工人数增加到 1 万人。

然而，与上传到 YouTube 的内容数量相比，这个数字微不足道。(根据[统计](https://web.archive.org/web/20230306043022/https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/)，截至 2015 年 7 月，每*分钟*就有 400 个小时的视频被上传到 YouTube 到目前为止，它很可能已经上升到每分钟 600 或 700 小时。)

YouTube 免费上传内容平台的庞大规模几乎使其不可能进行有意义的调整。

这是一个存在的问题，因为该平台的巨大规模、无处不在的跟踪和个性化的定位技术也赋予了它影响和塑造整个社会的力量。

该公司自己表示，其 10 亿多用户构成了整个互联网的三分之一。

委婉地说，考虑到谷歌对不干涉(即成本更低)的内容算法管理的偏好，以及其机器所做决策产生的一些社会影响是值得怀疑的。

事实上，YouTube 的算法被其员工描述为具有极端主义倾向。

该平台还被指责基本上自动化了在线激进化——通过将观众推向越来越极端和仇恨的观点。点击一个关于民粹主义右翼学者的视频，最终——通过算法暗示——被推向一个新纳粹仇恨团体。

该公司对这一人工智能极端主义问题的建议解决方案是什么？然而更多的人工智能…

然而，正是人工智能平台被发现放大假货，加速仇恨和刺激反社会情绪。

而且是人工智能驱动的调节系统太笨了，无法像人类一样判断上下文和理解细微差别。(或者至少*可以*当他们有足够的时间思考的时候。)

扎克伯格本人在一年前也说过同样的话，当时他的公司面临的生存危机的规模开始变得清晰。“值得注意的是，人工智能的重大进步需要理解文本、照片和视频，以判断它们是否包含仇恨言论、图形暴力、露骨的性内容等，”他当时写道。“按照我们目前的研究速度，我们希望在 2017 年开始处理其中的一些案件，但其他案件在很多年内都不可能。”

“许多年”是科技公司首席执行官对“实际上我们可能永远无法设计它”的说法。

如果你谈论的是非常困难、非常编辑化的内容审核问题，那么识别恐怖主义实际上是一个相对狭窄的挑战。

理解讽刺——或者甚至只是知道一段内容是否有任何内在价值与被[完全没有价值的算法修饰的垃圾](https://web.archive.org/web/20230306043022/https://techcrunch.com/2017/11/12/i-watched-1000-hours-of-youtube-kids-content-and-this-is-what-happened/)？坦率地说，我不会屏住呼吸等待能做到这一点的机器人。

尤其是在人们迫切要求科技公司展现更多人性的时候。科技公司仍在试图向我们强行灌输更多的人工智能。