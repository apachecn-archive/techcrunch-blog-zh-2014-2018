<html>
<head>
<title>Facebook is speeding up training for visual recognition models | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书正在加快视觉识别模型的训练</h1>
<blockquote>原文：<a href="https://web.archive.org/web/http://techcrunch.com/2017/06/08/facebook-is-speeding-up-training-for-visual-recognition-models/">https://web.archive.org/web/http://techcrunch.com/2017/06/08/facebook-is-speeding-up-training-for-visual-recognition-models/</a></blockquote><div><header class="article__header ">
	<p class="article__title-wrapper">
						</p><h1 class="article__title translated">脸书正在加快视觉识别模型的训练</h1>
		

			
	
			
	</header>

			<div class="article-content">
				<p id="speakable-summary" class="translated">花在训练深度学习模型上的每一分钟都是不做其他事情的一分钟，在当今快节奏的研究世界中，这一分钟非常有价值。脸书今天上午发表了一篇论文，详细阐述了其个人对这一问题的看法。该公司表示，它已经设法将 ImageNet 上的 ResNet-50 深度学习模型的培训时间从 29 小时减少到 1 小时。</p>
<p class="translated">脸书通过在数量更多的图形处理器上分配更大的“小批量”培训，成功地大幅减少了培训时间。在之前的基准测试案例中，256 个图像的批次分布在八个 GPU 上。但今天的工作涉及分布在 256 个 GPU 上的 8，192 个图像的批量大小。</p>
<p class="translated">大多数人没有 256 个 GPU，但大型科技公司和资金雄厚的研究小组有。能够在如此多的 GPU 之间扩展训练以减少训练时间，而不会大幅降低准确性，这是一件大事。</p>
<p class="translated">该团队在训练过程的开始阶段放慢了学习速度，以克服一些以前使大批量不可行的困难。为了不迷失在细节中，随机梯度下降用于训练 ResNet-50 模型。</p>
<p class="translated">随机梯度下降中的一个关键变量是学习率，即训练过程中权重变化的程度。这一变量随着迷你批量大小的变化而变化的方式是有效优化的关键。</p>
<p class="translated">机器学习开发人员整天都在处理妥协。更高的准确性通常需要更大的数据集，这需要额外的训练时间和计算资源。在这种情况下，有可能优先考虑准确性或速度以获得更令人印象深刻的结果，但在 20 秒内训练一个准确性差的模型并不是超级有价值的。</p>
<p class="translated">与大多数研究项目不同，脸书的人工智能研究(FAIR)和应用机器学习(AML)团队并肩工作，增加迷你批次的大小。从这里开始，小组计划调查今天工作中产生的一些额外问题。</p>
<p class="translated">“这项工作提出的问题比它回答的问题更多，”脸书反洗钱小组的成员 Pieter Noordhuis 说。“超过 8000 张图像后，错误率会再次上升，但我们不知道原因。”</p>
<p class="translated">脸书在这个实验中使用了其开源深度学习框架 Caffe2 和其大盆地 GPU 服务器。如果你想更深入地了解细节，可以点击这里获得来自脸书的更多信息。</p>
			</div>

			</div>    
</body>
</html>