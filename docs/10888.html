<html>
<head>
<title>Artificial intelligence and the law • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能与法律</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2017/01/28/artificial-intelligence-and-the-law/">https://web.archive.org/web/https://techcrunch.com/2017/01/28/artificial-intelligence-and-the-law/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary">
</p><div class="article__contributor-byline">
	

		<div class="contributor-byline__bio"><p class="translated">杰里米·埃尔曼是</p><a href="https://web.archive.org/web/20230221195800/https://www.dlapiper.com/en/us/">DLA Piper</a><p class="translated">，是DLA Piper Miami的知识产权和技术及新兴增长业务主管。</p></div>
	
	</div>

<div class="article__contributor-byline">
	<div class="contributor-byline__contributor">
		<p class="byline__author translated"><span class="byline__author-name">阿贝尔卡斯蒂利亚</span><span class="byline__author-title">撰稿人</span></p>

			</div>

		<div class="contributor-byline__bio"><p class="translated">阿贝尔·卡斯蒂利亚是一名软件工程师</p><a href="https://web.archive.org/web/20230221195800/https://www.codelitt.com/">Codelitt</a><p>.	</p></div>
	
	</div>

<p class="translated">法律控制着人类的行为，有时也控制着人类使用的机器，比如汽车。但是，当那些汽车变得像人一样时，就像可以驾驶汽车的人工智能一样，会发生什么呢？谁对人工智能违反的任何法律负责？</p>
<p class="translated">这篇文章由一位技术专家和一位律师撰写，探讨人工智能法律未来。</p>
<p class="translated">人工智能领域正在复兴，研究机构和R&amp;D巨头正在拓展人工智能的能力。尽管我们大多数人都没有意识到这一点，但人工智能系统无处不在，从让我们用一张图片存入支票的银行应用程序，到每个人都喜欢的Snapchat过滤器，再到我们的手持移动助手。</p>
<p class="translated">目前，人工智能研究人员正在应对的下一个大挑战是<i> <span>强化学习</span> </i> <span>，这是一种允许人工智能模型从其过去的经验中学习的训练方法。与其他生成人工智能模型的方法不同，强化学习更像科幻小说，而不是现实。通过强化学习，我们为我们的模型创建了一个分级系统，人工智能必须确定最佳的行动方案，以便获得高分。</span></p>
<p class="translated"><span>对复杂强化学习问题的研究表明，人工智能模型能够找到各种方法来实现积极的结果。在未来的几年里，可能会经常看到强化学习人工智能与更多的硬件和软件解决方案相结合，从人工智能控制的交通信号能够调整灯光定时以优化交通流量，到人工智能控制的无人机能够优化电机转速以稳定视频。</span></p>
<p class="translated">法律系统会如何对待强化学习？如果人工智能控制的交通信号知道比以前早一秒变灯是最有效的，但这导致更多的司机闯红灯，导致更多的事故，那该怎么办？</p>
<p class="translated">传统上，法律系统与机器人等软件的互动只在开发者疏忽或可以预见伤害的情况下才发现责任。比如<i> <span>琼斯诉W + M自动化公司</span> </i> <span>。在2007年纽约州的一个案例中，由于法院发现制造商遵守了法规，所以没有发现机器人龙门装载系统伤害工人的被告负有责任。</span></p>
<p>	</p><div class="article-block block--pullout block--right">
		<blockquote class="translated">我们不太可能进入一个反乌托邦式的未来，在那里，人工智能要为自己的行为负责。</blockquote>
	</div>
	
<p class="translated">但是在强化学习中，没有人的过错，也没有这种伤害的可预见性，所以传统的侵权法会说开发者不承担责任。那肯定会造成 <i> <span>【终结者】</span> </i> <span>【的危险，如果AI继续扩散而不承担责任的话。</span></p>
<p class="translated">在不久的将来，法律将需要适应这一技术变化。我们不太可能进入一个反乌托邦式的未来，在那里，人工智能要为自己的行为负责，被赋予人格，并被送上法庭。这将假定在普通法和世界各地各种法院中发展了500多年的法律制度将适应人工智能的新情况。</p>
<p class="translated">人工智能的设计是人为的，因此像责任或陪审团这样的想法似乎毫无意义。刑事法庭将与人工智能不兼容(除非开发者打算制造伤害，这将是其自身的罪行)。</p>
<p class="translated">但真正的问题是，如果出了问题，有人受伤，人工智能是否应该承担责任。这难道不是自然规律吗？我们不规范非人类的行为，像动物或植物或自然界的其他部分。蜜蜂不会蜇你。在考虑了法院系统的能力之后，最有可能的现实是，世界将需要采用一种人工智能标准，其中制造商和开发人员同意遵守一般的道德准则，例如通过条约或国际法规授权的技术标准。而且这个标准只有在可以预见算法和数据会造成伤害的时候才会应用。</p>
<p class="translated">这可能意味着召集一批领先的人工智能专家，如<a target="_blank" href="https://web.archive.org/web/20230221195800/https://www.crunchbase.com/organization/openai#/entity"> OpenAI </a>，并建立一个包括神经网络架构明确定义的标准(神经网络包含训练人工智能模型和解释人工智能模型的指令)，以及人工智能必须遵守的质量标准。</p>
<p class="translated">标准化理想的神经网络架构有些困难，因为有些架构比其他架构更好地处理某些任务。这种标准带来的最大好处之一是能够根据需要替换人工智能模型，而不会给开发人员带来太多麻烦。</p>
<p/>
<p class="translated">目前，从旨在识别人脸的人工智能转换到旨在理解人类语言的人工智能，需要对与之相关的神经网络进行彻底检查。虽然创建一个架构标准有很多好处，但许多研究人员会觉得在坚持标准的同时他们所能完成的有限，并且专有网络架构可能很常见，即使标准已经存在。但是，很可能会出现一些通用的道德准则，它们是由面向开发者的技术标准正式或非正式地传达的。</p>
<p class="translated">随着我们开始看到人工智能控制越来越多的硬件，对“质量”的关注，包括避免对人类的伤害，将会增加。并非所有的人工智能模型都是相同的，因为由两个不同的开发人员为同一任务创建的两个模型的工作方式会有很大的不同。训练一个人工智能会受到很多因素的影响，包括随机的机会。质量标准确保只有经过适当训练并按预期工作的人工智能模型才能进入市场。</p>
<p class="translated">要让这样的标准真正拥有任何权力，我们最有可能需要某种形式的政府干预，考虑到英国议会最近关于人工智能和机器人研究和应用的未来监管的谈话，这似乎并不太遥远。尽管还没有具体的计划出台，但议会似乎意识到有必要在该领域成熟之前制定法律法规。正如英国下议院科学技术委员会(House of Commons Science and Technology Committee)所说的那样，“尽管为这个新兴领域制定全行业的法规还为时过早，但现在就开始对人工智能系统的伦理、法律和社会层面进行仔细审查是至关重要的。”该文件还提到，当涉及到已部署的人工智能和相关后果时，需要“问责制”。</p>
			</div>

			</div>    
</body>
</html>