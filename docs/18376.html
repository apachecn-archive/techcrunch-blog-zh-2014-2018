<html>
<head>
<title>The problem with ‘explainable AI’ • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“可解释的人工智能”的问题</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/06/14/the-problem-with-explainable-ai/">https://web.archive.org/web/https://techcrunch.com/2018/06/14/the-problem-with-explainable-ai/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><span><div class="article__contributor-byline"><div class="contributor-byline__contributor"><p class="byline__author"><span class="byline__author-name">鲁迪娜·塞塞里</span><span class="byline__author-title">撰稿人</span></p></div><div class="contributor-byline__bio"><p>鲁迪娜·塞塞里是</p><a href="https://web.archive.org/web/20221121182629/http://www.glasswing.vc/">glass wing Ventures</a><p>的创始人和管理合伙人，是哈佛商学院的常驻企业家和哈佛大学创新实验室的常驻执行官。</p></div><div class="contributor-byline__more-articles"><span class="more-articles-title">更多帖子由本投稿人</span></div></div></span></p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">当讨论人工智能的透明度时，首先要考虑的应该是数据，这是为算法提供动力的燃料。公司应该披露他们从哪里以及如何获得用于推动其人工智能系统决策的数据。消费者应该拥有自己的数据，并应该对企业使用和出售此类信息的各种方式有所了解，这些方式往往是在没有明确和有意识的消费者同意的情况下进行的。因为数据是所有人工智能的基础，所以想知道数据来自哪里以及它如何解释人工智能系统做出的偏见和违反直觉的决定是有道理的。<u/>T29】</p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">在算法方面，IBM和其他科技巨头围绕“可解释的人工智能”的想法哗众取宠，只不过是没有现实基础的美德信号。例如，我不知道IBM在任何地方暴露了沃森的内部工作原理——那些算法是如何工作的？他们为什么会做出这样的建议/预测？<u/> <u/></p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">可解释人工智能的想法有两个问题。一个是定义:我们所说的可解释性是什么意思？我们想知道什么？使用的算法或统计模型？随着时间的推移，学习是如何改变参数的？某个预测的模型是什么样的？一种人类可理解概念的因果关系？</p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">其中每一个都需要不同程度的复杂性。其中一些非常简单——有人必须设计算法和数据模型，这样他们就知道他们使用了什么以及为什么使用。这些模型是什么，也是相当透明的。事实上，当前人工智能浪潮的一个令人耳目一新的方面是，大多数进步都是在同行评议的论文中取得的——对每个人开放和可用。<u/> <u/></p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">然而，这些模型的含义却是另一回事。这些模型是如何变化的，它们是如何对一个特定的预测起作用的，这些都是可以检验的，但是它们的含义对于我们大多数人来说是难以理解的。这就像买一台iPad，背面有一个解释微处理器和触摸屏如何工作的标签——祝你好运！然后，加上解决人类理解的因果关系，这是一个完全不同的问题。</p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">一些当前方法(最显著的是深度学习)的部分优势在于，模型识别(一些)比我们可以定义的变量更好的相关变量，因此它们的性能更好的部分原因与很难解释的复杂性有关，因为系统识别人类没有识别或表达的变量和关系。如果可以，我们会给它编程，称之为软件。<u/> <u/></p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">考虑可解释人工智能的第二个首要因素是评估“真正可解释和透明的人工智能”的权衡目前，在一些任务中，除了业务分支之外，还需要在性能和可解释性之间进行权衡。如果人工智能驱动的平台的所有内部工作都是公开的，那么知识产权作为一个区分因素就不复存在了。</p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">想象一下，如果一家初创公司创建了一个专有的人工智能系统，并被迫解释它是如何工作的，直到把它全部展示出来——这就类似于要求一家公司公开其源代码。如果这个IP有任何价值的话，公司在点击“发送”后很快就会完蛋。这就是为什么，一般来说，推动这些要求有利于拥有大量预算和市场主导地位的现任者，并将扼杀创业生态系统中的创新。<u/>T3】</p>
<p class="m_2577796187491844602m_1587208881082380096MsoPlainText translated">请不要把这误读为我赞成“黑盒”AI。公司应该对他们的数据透明，并向那些感兴趣的人提供关于他们的人工智能系统的解释，但我们需要考虑这是什么的社会影响，包括我们能做什么和我们创造什么样的商业环境。我完全支持开源和透明，并将人工智能视为具有积极影响的变革性技术。通过如此重视透明度，我们为这个新生但潜力巨大的行业设定了非常高的负担。</p>
			</div>

			</div>    
</body>
</html>