<html>
<head>
<title>Machine Learning And Human Bias: An Uneasy Pair • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和人类偏见:令人不安的一对TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2015/08/02/machine-learning-and-human-bias-an-uneasy-pair/">https://web.archive.org/web/https://techcrunch.com/2015/08/02/machine-learning-and-human-bias-an-uneasy-pair/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary">
</p><div class="article__contributor-byline">
	<div class="contributor-byline__contributor">
		<p class="byline__author translated"><span class="byline__author-name">杰森·鲍德里奇</span><span class="byline__author-title">撰稿人</span></p>

			</div>

		<div class="contributor-byline__bio"><p class="translated">贾森·鲍德里奇是</p><a href="https://web.archive.org/web/20220930001745/http://peoplepattern.com/">People Pattern</a><p class="translated">和副教授</p><a href="https://web.archive.org/web/20220930001745/http://www.utexas.edu/cola/depts/linguistics/">Department of Linguistics</a><p class="translated">在奥斯汀的德克萨斯大学。他的主要专业是计算语言学，他的核心研究兴趣是句法的形式和计算模型、句法和语篇结构的概率模型以及一般自然语言任务的机器学习。</p></div>
	
	</div>

<p class="translated">“我们在看着你。”这是芝加哥警察局向其“热门名单”上的400多人发出的警告这份名单旨在确定最有可能在这座城市实施暴力犯罪的人，它是通过一种预测算法创建的，该算法关注的因素包括，据<a target="_blank" href="https://web.archive.org/web/20220930001745/http://articles.chicagotribune.com/2013-08-21/news/ct-met-heat-list-20130821_1_chicago-police-commander-andrew-papachristos-heat-list">芝加哥论坛报</a>称，“他或她的熟人和他们的被捕历史——以及这些同伙中是否有人过去被枪杀过。”</p>
<p class="translated">像这样的算法显然会引发一些令人不安的问题。谁在这个名单上，为什么？它是否考虑了种族、性别、教育和其他个人因素？当美国的监狱人口绝大多数是黑人和拉丁裔男性时，基于关系的算法会不成比例地针对年轻的有色人种吗？</p>
<p class="translated">这类算法令人感兴趣的原因有很多，但回报与风险密不可分。人类是有偏见的，我们编码到机器中的偏见然后被缩放和自动化。这本质上并不坏(或好)，但它提出了一个问题:我们如何在一个越来越多地被“个人分析”消费的世界中运作，这些分析可以预测种族、宗教、性别、年龄、性取向、健康状况等等。</p>
<p>	</p><div class="article-block block--pullout block--left">
		<blockquote class="translated">人类是有偏见的，我们编码到机器中的偏见然后被缩放和自动化。</blockquote>
	</div>
	
<p class="translated">我敢打赌，大多数读者对芝加哥警察局热名单是如何实施的感到有点不安——即使他们同意算法背后的意图是好的。为了负责任地使用机器学习和公共数据，我们需要就<em>我们教给机器什么</em>以及<em>我们如何</em>使用输出进行令人不安的讨论。</p>
<h2 class="translated">我们教给机器的东西</h2>
<p class="translated">大多数人对种族、宗教和性别有着直觉的理解，然而当被要求准确定义它们时，他们很快发现自己很难确定它们。对于一个给定的人是什么种族，人类无法客观地达成一致。正如<a target="_blank" href="https://web.archive.org/web/20220930001745/http://scholar.harvard.edu/files/msen/files/race_causality.pdf"> Sen和Wasow (2014) </a>所言，种族是一种社会建构，基于可变和不可变特征的混合，包括肤色、宗教、地点和饮食。</p>
<p class="translated">因此，谁属于哪个种族类别的定义随着时间的推移而变化(例如，意大利人曾经被认为是美国南部的黑人)，一个特定的个人可能在某个时候认同一个种族，十年后又认同另一个种族。这种无法精确定义种族等概念的现象给个人分析带来了风险。</p>
<p class="translated">任何旨在预测、操纵和显示种族类别的程序都必须将它们用于内部处理和人类消费。机器学习是这样做的最有效的框架之一，因为机器学习程序从人类提供的例子中学习，而不是从显式规则和启发中学习。</p>
<p class="translated">因此，假设一名程序员构建了一个算法，该算法基于普通美国人已知的类别做出完美的种族预测——这被称为“常识测试”从其他角度来看，它的许多输出将是奇怪的。许多在自己国家被认为是白人的巴西人在美国会被认为是黑人。</p>
<p>	</p><div class="article-block block--pullout block--right">
		<blockquote class="translated">我们需要就我们教给机器什么以及我们如何使用输出进行一场令人不安的讨论。</blockquote>
	</div>
	
<p class="translated">混血儿美国人和来自印度、土耳其和以色列等地的个人经常挑战种族分类，至少美国人是这样理解的。因此，算法必然会将它的创造者的偏见操作化，而这些偏见会与其他人的偏见相冲突。</p>
<p class="translated">结果是一个机器学习程序像其创造者一样对待种族——<em>而不是</em>不一定像个人看待自己或程序用户看待种族那样。这在市场营销和社会科学研究等用例中可能相对没有问题，但对于芝加哥警察局的热点名单、“禁飞名单”和其他公共安全应用，偏见和误解可能会产生严重的大规模后果。</p>
<h2 class="translated">我们如何使用数据</h2>
<p class="translated">从<em>个人</em>的角度来看，任何用于个人分析的算法都会出错。一个人是多面的和复杂的，我们很少能整齐地归入清晰界定的群体。尽管如此，当个人层面的预测被<em>聚合</em>时，它们可以支持更好地理解大规模人群，帮助我们识别差异，并为如何更好地改造我们的社会提供更好的决策。</p>
<p class="translated">因此，如果敲潜在罪犯的门似乎是错误的，我们有其他选择吗？</p>
<p class="translated">利用芝加哥警察局的算法，一种选择是根据高风险人群和活动的位置生成“热图”。洛杉矶、亚特兰大、圣克鲁斯和许多其他警察管辖区已经使用一种名为<a target="_blank" href="https://web.archive.org/web/20220930001745/https://www.predpol.com/"> PredPol </a>的预测警务工具做了类似的事情。它允许警察部门在适当的时候增加他们在犯罪多发地区的存在，而不使用任何个人数据。它严格关注犯罪的类型、地点和时间。</p>
<p class="translated">但是，根据地点进行定性是另一种形式的歧视吗？警察会不可避免地在热图区域停下来给更多的人开罚单吗？如果我只能住在经济不景气的地区，我会比住在富裕地区的人更经常被警察拦下盘问吗？一个有针对性的、可预测的警察存在会不会将犯罪带到警察毫无准备的地方，从而扩大一个城市的犯罪地理范围？</p>
<p>	</p><div class="article-block block--pullout block--left">
		<blockquote class="translated">这是一个帮助而不是伤害人们的巨大机会。</blockquote>
	</div>
	
<p class="translated">相反，也许有一种净利益。通过战略性地部署警察并与社区合作，有机会减少犯罪并为居民创造更多机会。一个算法有可能比人类分析师辨别得更少。PredPol报告说，在实施该软件的城市中，犯罪率下降了两位数。芝加哥警方还没有发布任何关于热名单有效性的数据。</p>
<p class="translated">芝加哥PD和PredPol模型提醒人们，个人分析并不是唯一的选择。在我们实施身份之前——当然也是在我们瞄准个人并敲门之前——我们必须考虑我们的方法的道德性，而不仅仅是解决方案的优雅性。</p>
<h2 class="translated">禁忌，但是必要的</h2>
<p class="translated">谈论偏见令人不安，但我们不能忽视机器学习领域的这种对话。为了避免扩大刻板印象或侵犯个人权利，我们必须谈论这一点，因为它适用于每个旨在识别和分类人的机器学习算法。</p>
<p class="translated">这种算法的输入及其输出如何使用的透明度可能是这种努力的一个重要组成部分。像这样的伦理考虑最近被学术界认为是重要的问题:新的课程正在创建，像<a target="_blank" href="https://web.archive.org/web/20220930001745/http://www.fatml.org/#scope"> FAT-ML </a>这样的会议为关于该主题的论文和讨论提供了场所。</p>
<p class="translated">很容易想象芝加哥警察局的黑名单是如何被负责任地使用的。也很容易想象最坏的情况:如果参议员乔·麦卡锡在20世纪40年代末和50年代共产主义政治迫害期间能够接触到个人分析会怎么样？今天，如果有反同性恋和反变性法律的国家使用这种技术来识别和伤害LGBT个人会怎么样？</p>
<p class="translated">这些都是令人不安的场景，但还不足以成为埋葬这项技术的理由。这是一个帮助而不是伤害人们的巨大机会。使用机器学习，学者和政策制定者都可以提出重要的问题，并使用结果来指导对个人或社会规模有重大影响的决策。</p>
<p class="translated">像很多技术一样，机器学习本身是价值中立的，但最终的应用会反映创作者的问题、偏好和世界观。</p>
			</div>

			</div>    
</body>
</html>