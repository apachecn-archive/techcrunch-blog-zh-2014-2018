<html>
<head>
<title>Facebook's content moderation rules dubbed 'alarming' by child safety charity | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">儿童安全慈善机构| TechCrunch称脸书的内容审核规则“令人担忧”</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2017/05/22/facebooks-content-moderation-rules-dubbed-alarming-by-child-safety-charity/">https://web.archive.org/web/https://techcrunch.com/2017/05/22/facebooks-content-moderation-rules-dubbed-alarming-by-child-safety-charity/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">《卫报》公布了脸书内容审核指南的细节，涵盖了暴力、仇恨言论和自残等有争议的问题，这些内容摘自该报看到的100多份内部培训手册、电子表格和流程图。</p>
<p class="translated">这些文件以白纸黑字的形式列出了脸书在处理不同类型的令人不安的内容时采取的一些矛盾立场，它试图在删除内容和坚持其对“言论自由”的偏好之间取得平衡这在一定程度上解释了为什么该公司继续遇到适度问题。再加上它雇佣的审查和判断被标记内容的人数很少。</p>
<p class="translated">例如，内部审核指南显示，脸书允许分享一些非性虐待儿童的照片，如欺凌的描述，只有在被认为有虐待狂或庆祝成分的情况下，才会删除或标记内容。</p>
<p class="translated">脸书也乐于接受展示虐待动物的图像——只有那些被认为“极其令人不安”的内容才会被标记为令人不安。</p>
<p class="translated">该平台显然允许用户直播自残企图——因为它说它“不想审查或惩罚处于困境中的人。”</p>
<p class="translated">当谈到暴力内容时，脸书的指导方针允许暴力死亡的视频被分享，尽管被标记为令人不安，因为它说它们可以帮助建立对问题的意识。虽然某些类型的一般暴力的书面声明——例如鼓吹暴力侵害妇女的声明——被允许存在，但脸书的准则要求它认为“可信的行动呼吁”,以便删除暴力声明。</p>
<p class="translated">这些政策还包括如何处理复仇色情的指导方针。对于要删除的这类内容，脸书要求满足三个条件——包括版主可以通过“报复性上下文”或来自独立来源(如新闻报道)的“确认”缺乏同意。</p>
<p class="translated">根据《卫报》看到的一份泄露的内部文件，脸书不得不在一个月内评估接近<a target="_blank" href="https://web.archive.org/web/20230306025955/https://www.theguardian.com/news/2017/may/22/facebook-flooded-with-sextortion-and-revenge-porn-files-reveal?CMP=Share_iOSApp_Other" rel="noopener noreferrer"> 54，000起复仇色情</a>的潜在案件。</p>
<p class="translated">指南中的其他细节显示，任何拥有超过10万名粉丝的人都被指定为公众人物，因此被剥夺了给予个人的保护；脸书改变了裸体政策，此前该国决定删除一张描绘裸体儿童尖叫的标志性越战照片，引发强烈抗议。现在，在“战争恐怖”的指导方针下，它允许“有新闻价值的例外”。(尽管网站上不允许出现大屠杀背景下的儿童裸体图片。)</p>
<p class="translated">内部规则的曝光正值这家社交媒体巨头因其在内容审核方面做出的<a target="_blank" href="https://web.archive.org/web/20230306025955/https://techcrunch.com/2017/03/07/facebooks-content-moderation-system-under-fire-for-child-safety-failures/" rel="noopener noreferrer">决定</a>而面临<a target="_blank" href="https://web.archive.org/web/20230306025955/https://techcrunch.com/2017/04/13/facebook-under-fire-for-spreading-illegal-content/" rel="noopener noreferrer">越来越大的压力</a>。</p>
<p class="translated">例如，4月5日在T4，德国政府支持对社交媒体平台未能及时删除非法仇恨言论处以高达5000万€的罚款。英国议会委员会本月也呼吁政府考虑对内容审核失败处以罚款。同时，本月早些时候，一家奥地利法院裁定脸书必须删除被认为是仇恨言论的帖子——而且是在全球范围内，而不仅仅是在本地屏蔽这些帖子。</p>
<p class="translated">与此同时，脸书的直播功能已被用于广播谋杀和自杀事件，该公司显然无法先发制人地关闭流媒体。</p>
<p class="translated">在脸书直播出现问题后，本月早些时候，该公司表示将<a target="_blank" href="https://web.archive.org/web/20230306025955/https://techcrunch.com/2017/05/03/facebook-to-hire-3000-to-review-posts-with-hate-speech-crimes-and-other-harming-posts/" rel="noopener noreferrer">额外雇佣3000名版主</a>——使其审核帖子的总人数达到7500人。然而，对于一个拥有近20亿用户、每天分享数十亿条内容的服务来说，这只是沧海一粟。</p>
<p class="translated">当被问及对脸书的适度指导方针的回应时，英国国家防止虐待儿童协会的一名发言人形容这些规定“令人担忧”，并呼吁对该平台的适度政策进行独立监管——并对不合规者进行罚款。</p>
<p>	</p><div class="article-block block--pullout block--left">
		<blockquote class="translated">社交媒体公司……需要被独立监管，并在未能保护儿童安全时被罚款。</blockquote>
	</div>
	
<p class="translated">这位发言人告诉我们，“至少可以说，这种对脸书限制内容规则的洞察是令人担忧的。”。“脸书可以做更多的事情来保护他们网站上的儿童。脸书和其他社交媒体公司需要受到独立监管，并在未能保护儿童安全时被罚款。”</p>
<p class="translated">在回应《卫报》报道的声明中，脸书全球政策管理负责人莫妮卡·比克特说:“保证脸书人民的安全是我们所做的最重要的事情。我们努力让脸书变得尽可能安全，同时保证言论自由。这需要对详细且通常困难的问题进行大量思考，并且我们非常认真地对待正确的问题。马克·扎克伯格(Mark Zuckerberg)最近宣布，在未来一年，我们将在全球的社区运营团队中增加3000名员工，以审核我们每周收到的数百万份报告，并快速改进流程。”</p>
<p class="translated">她还表示，脸书正在投资技术，以改善其内容审查流程，包括研究如何进一步自动化内容审查——尽管它目前主要使用自动化来协助人工内容审查。</p>
<p class="translated">“除了投资更多的人，我们还在建立更好的工具来保护我们的社区安全，”她说。“我们将让向我们报告问题变得更简单，让我们的审查者更快地确定哪些帖子违反了我们的标准，如果有人需要帮助，他们也更容易联系执法部门。”</p>
<p class="translated">首席执行官马克·扎克伯格之前曾谈过使用人工智能来帮助大规模解析和调节内容——尽管他也警告说这种技术可能还需要几年时间。</p>
<p class="translated">脸书显然把它对未来自动化所带来的大量内容审核问题的长期希望寄托在这上面。然而，算法可以智能地判断人类的复杂性，比如什么时候裸体合适，什么时候不合适，这种想法在很大程度上是技术乌托邦主义者的信条。</p>
<p class="translated">对脸书来说，更严峻的政治现实是，来自对其当前内容审核失败的强烈抗议的压力，将迫使它在短期内雇佣更多的人来清理其行为。</p>
<p class="translated">此外，正如这些内部适度指导方针所显示的，脸书自己的立场显然是想在开放/自由表达与“安全”之间寻求平衡，这本身就是矛盾的——这正是它在内容适度争议中遇到的各种问题。</p>
<p class="translated">例如，对脸书来说，禁止所有展示虐待动物的图片相对容易，但这样的立场对脸书来说显然“太安全了”。或者更确切地说，它成为全球共享平台的雄心受到了太多限制。毕竟，每一个被踢的狗的视频都是脸书赚钱的内容。可以肯定地说，接受这个令人不安的事实只会让脸书更加不舒服。</p>
<p class="translated">在这篇报道中,《卫报》引用了一位内容审查专家Sarah T Roberts的话，她认为脸书的内容审查问题是其庞大“社区”的结果“当你是一个拥有一群分享原则和价值观的人的小型网络社区时，这是一回事，但当你拥有世界上很大一部分人口并说‘分享你自己’时，你就会陷入混乱，”她说。"那么，当你将这种做法货币化时，你就进入了一个灾难性的境地."</p>
<p class="translated"><strong>更新:</strong>英国心理健康慈善机构<a target="_blank" href="https://web.archive.org/web/20230306025955/https://www.mind.org.uk/information-support/tips-for-everyday-living/online-safety-and-support/overview/#.WSMN4pq6xE5" rel="noopener noreferrer"> Mind </a>的数字负责人Eve Critchley也对脸书的指导方针做出了回应，他说该组织担心该平台做得不够。“他们必须认识到自己在应对高风险内容方面的责任，这一点很重要。她告诉我们，虽然脸书实施了让版主在担心某人的安全时升级局势的政策，这是积极的，但我们仍然担心他们不够强大。</p>
<p class="translated">“将人们自残或自杀的经历进行分流是一个极其敏感和复杂的问题，”她补充道。“我们还不知道在社交媒体平台上分享这些材料对公众，特别是对那些可能正在与自己的心理健康作斗争的弱势群体的长期影响。我们所知道的是，有大量证据表明，媒体对这种行为的生动描述可能对观众非常有害，并可能导致模仿行为。因此，我们认为社交媒体不应该提供一个平台来传播人们伤害自己的内容。</p>
<p class="translated">“社交媒体可以以积极的方式使用，可以在一个人更广泛的支持网络中发挥真正有用的作用，但它也可能带来风险。我们不能假设一个人的社区会有必要的知识或理解，或者会在他们的反应中有同情心。我们还担心，对观看者的影响不仅会令人不安，还可能对他们自己的心理健康有害。</p>
<p class="translated">“脸书和其他社交媒体网站必须紧急探索使其在线空间安全和支持的方法。我们鼓励任何管理或主持在线社区的人，在适当的时候，引导用户找到紧急帮助的来源，如Mind、Samaritans或999。</p>
			</div>

			</div>    
</body>
</html>